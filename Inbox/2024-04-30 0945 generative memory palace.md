
>[!tip] Created: [2024-04-30 Tue 09:45]

>[!question] Targets: 

>[!danger] Depends: 

Be able to have meetings in an org, and have the knowledge transfer go out to everyone by way of the reconciliation with the company knowledge base.  Can always trace back to the source of the knowledge, inconsistencies and clarifications can be automatically sought.

Makes knowledge disemination effortless.  

Tying memories or concepts to objects should be easy in metaverse.  The object gives you a handle on the memory.  The object can be picked from a massive standard library.  When 3D generation comes on the scene, we will be poised.

Be able to zoom in and out of worlds.

Webpages and documents can be laid out in space, snapshot the webpage.  Zoom in to a small arrangement like stonehenge so can work on it in detail, then zoom out to present in a landscape to the user.

Could offer computers that can spin up to run conventional apps for users, where it acts like github codespaces.

Streaming 3D, so a compute cluster can generate complex geometry, or dynamic changing geometry, and it gets streamed into the headset, so all it has to do is render the scene.  Cluster does geometry chunking down, so it can summarize far away objects to reduce render load.
Gradual loading data means low poly versions can load fast and then as more data comes in, higher rez versions.  The geometry calculations should be taking place off headset as much as possible.