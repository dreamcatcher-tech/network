
>[!tip] Created: [2023-12-29 Fri 12:29]

>[!question] Targets: 

>[!danger] Depends: 

If a database was made in pseudo-schema, and parsed by AI, then creating new complex systems becomes more accessible to people since structure is really only required for machines to parse the information.  But if the machine can parse natural language, then why bother with the painful precision of the schema ?

Storing text is the most compact form, as structure costs storage.
Freeform means that the schema can be updated on the fly at any point with no release cycles required.
New data can be ingested without any formatting or translation required.

Objects can be assembled based on some basic structures.  Collections and records, where they can be linked and nested.
Updates are trivial, even in bulk.
Indexes can be built where queries are run using AI like a lens.
Template.
Rules that apply down the tree.
Filesystem relationships in a DAG.

Text can then be stored as git diffs, rather than full copies each change, or any other complicated structure.

Make a simpler model that is called by a single function - the record id, followed by the operation, and then a single gpt3.5 model does the operation, rather than having a big bunch of context that could possibly confuse it.  So it makes a special function that has a standard signature that runs via AI.  This means we don't have to code it, can test it thoroughly on all invocations.

The output of the function shouldn't change over time, like a brittle piece of code, since the natural language results would always be the same.  And so we will have an npm library of these kinds of functions that are certified and tested and run on these certified models, removing the need for software checks since you cannot exploit like you can when running code.  Supply chain not a problem, licensing not a problem.

Using hashes as record IDs means the model should find it harder to make a mistake.

So if we assemble simple lensing and generate types of queries that move the cursor around in predictable way, like walk, map, and others, we may not need any schema at all, and our software toolchain becomes very very small, where it only need to run an LLM to be able to do all kinds of advanced things.  So the app would simply download an LLM or use the GPU chain, and then the performance depends on how much GPU power you have, where it can run many queries in parallel.

So use gpt3.5 as an example, and make lensing primitives to be able to walk over a dataset and perform some very basic operations.  Provide tests for these 'kernel' operations, and allow people to write their own ones, in NL.

There may be a need for some quick search tools, like if a text file has a mapping in it, then we might do a text search to find the line and then get the link that was being referenced.  This would be a set of core functions that operate on the contents of indexes, which can be backed by chains if we wish, but from the LLMs point of view, and just function calls.

The LLMs should be set up to deal with everything by line number and row, so that files can be edited precisely.  These should be atomic in that the LLM should make edits only if the file hasn't changed, or, the pieces the LLM read from haven't changed.

Schema was a good way to ensure format correctness, but if the LLM performed well enough, what is this even for ?  If it errors, we can replay due to blockchain and analyze the response.  The beauty of this system is that the LLM can only error in certain ways, and so a fault in the llm that gets fixed, improves every other operation.

Ultimately once we have enough of these queries, we can start to tune the model to use them, reducing the need for helps, and having it built in to the training set.  If it can be made inately aware of the history based model, and the multiverse way of viewing data, then we can avoid helps that force this behaviour ?

How does this affect our stuckloop model ?  This should require even less code, since fixes can be applied in natural language, and therefore be generated by a human directly, or generated by an AI that was guided by a human, or purely generated by AI.

If the programming language is natural language, then anyone can do it, surely ?

? how to make a stuck NFT using this system ?
? how to do truck routing with this system ?

Optimization should be possible with a background thread that is always trying to make versions of peoples queries faster and more efficient, sometimes by changing the format their data is stored in.

Loop around the format checker so we can ask the same question in different ways to be assured of correctness of the data or the transform that was asked for.
# Example

In the CRM, we have a minimum information requirement. We can set some rules at the top of the system that say what the minimum is required, then we can walk the collection and check each one.  We can tolerate dirty data.


## GUI
What should the stateboard show in this way of working ?
Text diffing would be useful.
Scrolling back thru the documents of focus would be useful.
So each thing you looked at is presented as a panel and you can scroll back to see the chat about that object and also the history of what you were looking at, making comparisons easier.
GUI is just another text file that gets update on the fly that we interpret into visuals, like comparing something side by side.

Store the text as markdown so we can display it nicely would also help.
