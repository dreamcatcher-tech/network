
>[!tip] Created: [2025-02-22 Sat 13:44]

>[!question] Targets: 

>[!danger] Depends: 

The browser would set what it wants to listen to, which would expire, and then the workers check this based on the messages they are processing, by way of some kind of ID relating to the top level job.

The workers would then write to the db with the chunks, in order, and the browser would connect to an isolate, that does a watch on these chunks, and the watcher ensures they get assembled correctly and sent back down.

It might get costly, since it would be $3.50 per million kb of text chunks, so per GB of streamed text.

So we would only turn it on when it was needed explicitly, and when first turned on, the worker would make the first chunk be a catch up chunk.

The latency might be of concern, but it would still be streaming, and we can smooth out the stream some, like packing in the writes to be more optimal.

So when browser wants to subscribe, it would make a request with the job it wanted, and then receive SSE streams for jobs in flight, as well as commits relating to the job which it could read fully.

Charge the users for this extra watching, since it is expensive.

Means that multiple users can watch the same streaming progress.

If the receiving isolate did the AI calls then the streaming becomes trivial, so this should be the norm.

Another way, using sqs queues, would push the messages onto the sqs queue, and then use SNS to filter the queue messages ?

Could use AWS API gateway, where the deno session registers with the service, and then the api gateway routes all messages for a given registration to those subscribers.

AWS seems at best $1 per mission messages, plus the integration overhead.

Could run a fly.io container that stops when idle, but that just does match making, between incoming connections and browser connections, where the browser connects to deno, then deno connects to the fly.io container to receive its messages.  Using deno in this container means it should handle huge concurrent connections, and being only available to deploy instances makes it safe.  This seems less wasteful that slamming the db repeatedly.  Would be the fastest thing possible.  If received using h3, then should be very low latency and no need for websockets.

What about just using the broadcast channel to send out messages, and the browser is just listening on this channel, and signals in the db that they want to receive these things as streams ?
Then we push best effort chunks in there, and allow specific catchup messages to be broadcast.

## Tee on the bucket writes
If all bucket writes went via some tapped host that was a container, then we can tee off the stream.
If the files are large, we can use liveread from backblaze so we don't have huge amounts in ram, or we can flush to b2 every 10MB or so, then just copy it over.

So clients that want to tap the ephemeral tips can just tap this stream.

This service can in fact be made generic over all tip operations, since it means that an isolate need not keep the tip in memory, and it need not do the hassle of compression.

Optimized for prepping for git under the hood, optimistic that a write will be committed.
So while the stream is coming in, provided the sender told us how long, it can be getting compressed.
We might be able to compress it and then tack the length on the front after.
Or we can write to the bucket using our own custom compression, and the regenerate the length while pulling it off disk ?
Just means that we should be able to make a commit go faster with this tee pattern.
As the bandwidth node of a t-container gets high, we just online more of them.  A single node could be the tee for a large number of jobs.  Tee handoff can be when some kind of a router service points incoming requests at a given tee node id.

If we don't know the size then we can at least upload to b2 live, whilst allowing a tap, then use the b2 copy function to move the data around b2 side.

This same feature is used for realtime voice relay.  Treats the tip like a global filesystem.  Writable to only the owner, but readable to all.

We will always need huge bursts of compute, but we will also need stable writes, and listening in on writes, reliably.

On crash of an isolate, the revived isolate, since repeatable, should produce the same write signatures, so we just wait for that to catch up, and keep playing to the listeners.

In a blockchain, even these streams could be based on consensus ?  Or at least, you can follow different validator versions of this tip.  You should have to pay for this realtime tee.

The issue with blocking is that we need to buffer, rather than stream the packets, so the streaming layer needs to be an overlay, and is specific to a given node.  You should be able to pull from multiple validators executing the same thing, and compare the streams, but you shouldn't have to wait.  Even if the info is wrong or disputed, having an indication of it is valuable, and in most cases it won't be wrong.

## b2 live read hack
In b2, multipart allows arbitrarily small sizes below the chunk size for the final part of the multipart.  So we could do a liveread, and only make use of the first part. 
This means we have liveread for arbitrarily small files, and we have streaming of the partial writes.

An isolate could opt to write in this live readable way, since it would cost more.  Only if the liveread is being watched should we do this.

In b2, we could store items backwards, where we then reassemble them into git format on read.  We could generate a stream that is correct.  So the end of the file is always its length, and the body is compressed using our custom algo, which might include some deduplication.

Then when it gets read, we read the tail first, then start returning back the file, possibly passing it thru a very light zlib compatibility, so that git can read it.  Or we patch the git library to just read in uncompressed.

Best is to use custom compression on the stream on the way thru, then store the git header last, using a null byte to demarcate.  When reading, grab this first, and create the compression stream on the fly.  Compression ratio seems cheaper than the extra cpu needed to do the streams simultaneously.

This process should also allow for seek reads too, where we only want a part of a file.

For this, we might store using some bencode so we can provide chunk markers of sorts ?