
>[!tip] Created: [2025-02-22 Sat 13:44]

>[!question] Targets: 

>[!danger] Depends: 

The browser would set what it wants to listen to, which would expire, and then the workers check this based on the messages they are processing, by way of some kind of ID relating to the top level job.

The workers would then write to the db with the chunks, in order, and the browser would connect to an isolate, that does a watch on these chunks, and the watcher ensures they get assembled correctly and sent back down.

It might get costly, since it would be $3.50 per million kb of text chunks, so per GB of streamed text.

So we would only turn it on when it was needed explicitly, and when first turned on, the worker would make the first chunk be a catch up chunk.

The latency might be of concern, but it would still be streaming, and we can smooth out the stream some, like packing in the writes to be more optimal.

So when browser wants to subscribe, it would make a request with the job it wanted, and then receive SSE streams for jobs in flight, as well as commits relating to the job which it could read fully.

Charge the users for this extra watching, since it is expensive.

Means that multiple users can watch the same streaming progress.

If the receiving isolate did the AI calls then the streaming becomes trivial, so this should be the norm.

Another way, using sqs queues, would push the messages onto the sqs queue, and then use SNS to filter the queue messages ?

Could use AWS API gateway, where the deno session registers with the service, and then the api gateway routes all messages for a given registration to those subscribers.

AWS seems at best $1 per mission messages, plus the integration overhead.

Could run a fly.io container that stops when idle, but that just does match making, between incoming connections and browser connections, where the browser connects to deno, then deno connects to the fly.io container to receive its messages.  Using deno in this container means it should handle huge concurrent connections, and being only available to deploy instances makes it safe.  This seems less wasteful that slamming the db repeatedly.  Would be the fastest thing possible.  If received using h3, then should be very low latency and no need for websockets.

What about just using the broadcast channel to send out messages, and the browser is just listening on this channel, and signals in the db that they want to receive these things as streams ?
Then we push best effort chunks in there, and allow specific catchup messages to be broadcast.

