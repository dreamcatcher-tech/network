
>[!tip] Created: [2025-01-03 Fri 13:48]

>[!question] Targets: 

>[!danger] Depends: 

And so the issue seems to be when generating with O1 Pro that if it has ancillary files within the prompt and the files are wrong it will be unable to ignore them so it can't seem to ignore things when it's told and so this means that we might need to make a dedicated step where it selects what to ignore and then it blinks them out of its mind before answering the question because it seems to be unable to handle this internally.

Also, being able to make top-level guidance documents, blocking out everything else, and then maybe using reference documents like prompts and rules to guide that, can help roll down the stack. So basically, we're just top-level thinking, apply the reasoning, and then low-level thinking, where we're following out the commands, as opposed to trying to do everything all at once, or do multiple features at once.

Also can't ignore content well, so things like issues, need to narrow those down to just little pieces.  Uncertainty kills it, so can't be vague about things.

Asking leading questions is better than asking for a final solution, since the lead up serves to allow us to check its thinking, and redo the prompt each time if we want changes, before getting it to do the final work in some kind of format.

Formats and rules seem to confuse it, as its best responses come when it is most free.

Might be easiest to talk about processes in terms of math formulas, then alter the math formula to ensure it mentions or covers all the requirements.  The requirements might be just a list of statements, and we take the statements, and ensure they fit the math formula, then we write all the other files like README and process.md and ultimately the code implementation.

Seems it has hallucinations when being asked to write a  multi file solution - can mix up variable names.  Any task that requires coordination, like a front end and a back end, makes it easier that it will get things wrong.

The longer the generation, the more the chance it will go astray.

If you want a small change to something, it is best as a follow on question, since the same question twice will generate slightly different code, some of which you won't like.

Better to work from ground up, define the modules, then ask for them to be consumed, rather than going top down.  Top down should be debated about the architectural approach, and then built bottom up, with an arch reassessment at each level.

Something about the accuracy of the bot means for some tasks we don't need to check if it did it right ?  max speed is about knowing what to trust it to have done.

Do the fine check last - skim over the outputs and keep refining everything, like variable names and architecture.

variable names should be done as a follow up pass, so the structure and correctness is done first, and then the variables names is a fine adjustment that needs a reference, rather than a fresh generation.

Always be trying to get to stage where you can ask the bot a question - minmally that seems to be interface definitions, and some docs.

o1 stops us making mistakes that accumulate, like a class file that we iterated on and bent the internal structure of - it approaches everything as new and so the outcome is more succinct, but we do need to iterate rather hard on it.

No point going thru the minutia of each file - just get them largely right, then ask the bot to fix the errors in them, once you have bound the interfaces.

Get chats to be summarized as requirements, which should be honoured whenever regenerating a code file.  Requirements extrraction is key, since the bots should challenge or apply any requirements they see in some files.  A top level refactor that reorganizes the stub files is useful sometimes too, since a new architecture can reduce complexity.  We should be always extracting both the requirements of the code we're on, the project we're in, and also coding style and solution style.

We should be able to suggest a new strategy and then the workbench should reconfigure to run this thing every time, and start the process of self evaluating, where it can ask for some help, but it looks at things you've done before, and that others have done, and tunes itself to be cost effective and also meet your requirements.  You would give it a price limit so it didn't waste tokens.

The principle of the workbench is that we should never have to give the same instructions twice.  There should be a dream cycle, where when the human sleeps, the machine does batch processing to make sure it hasn't missed anything at all, and that it has interpreted all instructions correctly, and can reference them when asked about them. 