
>[!tip] Created: [2025-02-17 Mon 09:40]

>[!question] Targets: 

>[!danger] Depends: 

If plex was made serverless, then sharing media could be scalable, and so you could share on a per media file model.  So with the share comes the resources to play it, stream it, transcode it.

In the serverless world, this means that plex has zero idle cost.  Also transcoding of the same files across multiple accounts results in the same torrent chunks being shared.  So its basically a torrent transcoding service.

People pay to access the chunks.

Should be able to do adversarial modelling to ensure enough safeguards are in place, should the content be leaked.

Ideally an artifact powered plex system would allow remote control of any number of players and servers.
It would also allow currency exchange between providers of chunks of files.

Being controlled by an AI chatbot, with instant agent integrations, means that it can trigger things, or can be integrated into your smart home systems easily.

Using the artifact and dreamcatcher components, we can show how we can build a plex system, but also how a video editing and splicing system can be made, with huge storage, good scrubbing performance, local shared caches, devices with large SSDs that can offer network wide cache sharing.  All these things apply to plex, with private networks between friends, social discovery based on trust, clean handover between networks, internal network rewards, and global financial rewards.

Plus the editor being AI controlled, plus generative AI systems being included too.  Means also can be highly collaborative, and can offer a marketplace for content or editing.

Thats real good for processing, where the processing can take a long time, but if we were actually connected to a render farm, and a hugh workload was being processed.  Then this is the same as plex transcoding, and we don't really care what the data is, we just do what we're told.  So the browser based form would perform better since is data is more accessible in a datacentre.  This follows the general form of data processing using artifact, and is a good use case of this system.

Map out the legal ramifications and requirements of this, which if public, might require some kind of content detection framework.  Then private networks can overcome this, but the general offering and software maintenance part uses this clean data.  Plus much content needs to be private, so these direct contracts, with liability on the user, not the provider, are what matters.  Plus if we offer enclaves, the remote machines can genuinely deny knowledge.

Video is good since the plugins and tools are always vast, and so making the whole platform be plugins, plus agents, plus human workers, is good.  Also the editors always highly customize their environments.  So they could ask for their own special tooling easily.  Drive their own workflows using agents.  Basically it would be an editor platform for agents, where agents can connect up to the data and drive it, with the same guarantees of private data.

The compute share part means your computer can run locally whatever it can manage, but where possible it will use remote resources, which could be other computers in the building, dedicated servers, or shared rentable servers from trusted providers.

Versioning is handled automatically.  Because of this rich history, agents can look at it, and discern what you might like to do next, plus knows what other editors are doing, either popular or your friends, so it can show where they might do something different, so you can learn from each other in this distant way.

So it isn't about us providing a hosted service, its about providing software that lets people run their own, and then share that with each other, both publicly and in friend circles, then we offer a solid base service, but primarily we take a cut on usage, which drives us to keep our prices low, and lets people use their own equipment.  sharing is the solution to the environmental problems we have.

can torrents be made serverless too ?