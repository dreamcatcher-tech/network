
>[!tip] Created: [2025-03-22 Sat 10:36]

>[!question] Targets: 

>[!danger] Depends: 

So when the model is trying to understand a complex topic, it might generate snippets, or little nugets to verify its understanding.

Humans could flick thru these, and swipe left or right based on correctness, or give some voice feedback to drill in to each one.

Seems easier than digesting huge chunks of text.

Test these top level statements, and also compare between team members to check for alignment, and drill in to minute differences, which lead to bigger differences if unaddressed.

Models could be trained to splatter across the space, so that they are getting almost a monte carlo style coverage form the human.

They are also self policing, where they improve their own knowledge by considering what they have output.

These snippets could then be used as a fine tuning process.  As well as improving strategies that use these models.

Each upgrade or change would run against the prior results of these flashcards to check they produce the right thing.

Using this thinking tuning that openai has released, we could reinforce the reasoning path that was taken to generate the cards the human liked.