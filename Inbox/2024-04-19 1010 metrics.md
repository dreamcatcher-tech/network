
>[!tip] Created: [2024-04-19 Fri 10:10]

>[!question] Targets: 

>[!danger] Depends: 

Can the metrics be put into a chain periodically ?
When each bucket closes, a git commit is triggered.
First write to a new bucket triggers a delayed queue action to close out the bucket when its done.

Could be a separate branch on the main account, used for stats tracking.  Lets us purge it periodically.  Summarize the stats periodically.

Because each queue message results in a commit being done, we could store the metrics in the commit itself, like the atomic retry number, the time to process, the delay time since the message was enqueued ?

If the commits held the metrics, then reading the metrics is probably a bit of a heavy load.
Some accounting should be added there, so people can audit where the costs went, but timing is eaiser in a centralized db ?
Stamp the queue depth into the commit too.
If the summary db is used to handle high volume viewing, and the commits are used for detailed analysis of a specific commit, we should be able to trace down anything.

Use sound to indicate metrics - errors as a click / crackle, normal activity like a gentle hum.
pool and execute as a different tone
branch and merge as opposing tones

Cache hit metrics for the gitkv store, and the web cache.

Undelivered messages stat.

Total bytes stored metric, also stored by repo

github as an api gateway service, so we can log bytes sent, latency, and other stats with each interaction, so the gateway chain can be interrogated for metrics, as well as sending data into the metrics collector for the api gateway, since all have the same interest - the gateway is effectively the network card of Artifact.

Using kvtoolbox, we can fetch size info before fetching the item, so we know if it will be prohibitively large.

Store a flag that is true if the account is out of credits, so we can check for nullity, but if the account runs out of credits it goes into throttle mode.

Throttle means you can no longer access your repos, but you can export them to github, for example.  Other users who have good account standing can use your apps, since they can pay for the execution themselves.

Once account is locked, then data exports are all that is allowed, up to some limit.
Or deletion.

We should be able to tie AI credits and KV credits to the same api gateway system, so any app can be charged with the same way.

Charge for each commit and the size of the commit.
Refund for data deleted.
Executions should cost, based on cpu time taken, but we can just charge per invocation currently.
So each commit costs money as a base free, then a storage fee is added too.
Storage is the forever price.
Reading data costs based on size of data.
Web page loads, when the bots are published.
Input tokens from humans, including average size of prompt inputs.
Impact thruput - value created metrics.
Stucks rate - how frequently are new stucks being generated.
Help rate - how frequentyly are stucks being solved with helps.
Active users.

Publish platform level metrics about time to commit normalized by size.
Publish read rates for all operations.
AI token rate, and AI token latencies.
Prepayments in the system.
Roll up trusted deployments that are running independently.

If each terminal tracked its token usage, db usage.
Further each machine could track the platform, ip addresses, other data.

Screen rez per session would be helpful to track as well.

Use gigabytes of json under management, as well as stats on binary.
The more data we are managing, the more valued the chain is.
Also can score this based on consensus levels, so multiparty consensus, commercial grade hosting, etc.

Transaction processing in terms of tx / sec but also GB/sec - how much data is being processed by the system.

Long running operations, so we can measure the time to do a commit based on the number of commits that have already occured.
Could just run ping pong for as far as it can get in a certain time, and see what the result was.

Metrics for charging for git operations should include timing information so we can gather performance stats, and can know the hotspots in our app.

Developers of isolates should be able to see the historical calls that their isolate was involved in, and the timing information of all of these showing per line, which is used for attribution but also for hotspotting the app, and seeing what operations cost the most, so that devs can optimize.