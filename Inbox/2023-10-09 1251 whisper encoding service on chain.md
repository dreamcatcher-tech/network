
>[!tip] Created: [2023-10-09 Mon 12:51]

>[!question] Targets: 

>[!danger] Depends: 

Be able to load up all our conversations, then run them thru a processing pipeline on whisper models, with transcriptions.  Permit editing and retranscription to be done later when better models come, or humans come along.  Use whisper as the gatekeeper, so if someone proposes a change that couldn't possibly be right, then refuse to accept it, otherwise soft accept.  Possibly improve whisper in this way.

Make a listening app that plays them but also shows the transcriptions and allows corrections.  It then parses the key concepts, and links to packets that were based on these snippets, giving a relative score as to who came up with what idea, and how much.

It would make a pallette of you most frequently used actions or emergency actions that you asked for, so the interface is always changing to fit you.  Paints a picture of what the interface looks like, then uses object recognition to define the touch sensitive area.

The first loop as to a debate about the output of the attributor is how we learn how that loop goes, and how it can be managed using natural language.

Should be able to navigate these apps entirely by voice, but not to click buttons - to actually do things, potentially new things that they haven't done before.  Then can walk around with a mask capturing voice and only a phone to show summarizations as vast info is searched and processed, and instructions on capital placement handled.

Can use this whisper service to transcribe phone calls in realtime, and to continually parse those thru gpt4, and pipe back into any given app onsite to decide what to do next.