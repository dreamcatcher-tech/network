
>[!tip] Created: [2025-05-07 Wed 14:10]

>[!question] Targets: 

>[!danger] Depends: 

If we provide a UI to an agent operator and then ask it where best it would think we should put a certain feature, it should be able to appraise the application and make a bunch of shortlisted choices on how the app could be evolved to fit the new need. 

Design gets exponentially more complex as more features get added, and it can take a lot of human time to test and figure out where these features should go. Bots trying to do one-shot guesses at it are not very good, but an operator that can do usability measurements such as number of clicks, distance, the mouse travel clutter on screen, maximum information content where the key items are always visible at all times. These timescores allow it to experiment with different options that might suggest but also allow the builder to propose builds and then the agent checks them. 

The agent would at the same time be doing QA exercises to make sure every aspect of the application was checked, saving a human the monotony of doing it. In aggregate, these operator sessions can be looked at and a short list of efficient tests can be deduced. These stories that are actually implemented can be reverse out which makes a cleaner way to describe the stories of the system. 