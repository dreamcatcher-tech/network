
>[!tip] Created: [2022-12-08 Thu 18:56]

>[!question] Targets: 

>[!danger] Depends: 

Dreamcatcher as a set of software tools, management practices, and professional services that allow apps to be built rapidly, safely, cheaply.

Not just paying for apps by only your usage of only the code you used, but getting paid directly for this usage, at the instant it happens.

Make a complex product with transparent operation, then watch people build on top of it.

As a standalone blockchain, if taken to conclusion as best we can see it now, it might be good enough to be a solution to the general purpose blockchain problem if we're super luck.  However, it is good enough in its current form to better solve the innovation problem.  If it can cause a rate of innovation rate increase (ie: innovation on innovation rate), then that innovation power will improve this model to be the solution to general purpose blockchain.

Internet was made like a phone system with computers plugged in to it, however the ideal model is it acts like a computer, with people and things interacting with the computers.  The survivability has always been about the phone system, with each computing service left to its own devices to survive computation faults in all their forms.

Focusing on DX and BX, where BX is the business experience.

Innovating on innovation.

To pull in those struggling in the commercial world with almost popular software, and to lift up those struggling in the open source world with almost funded software.  By offering a business model to open source that aren't working yet, and commercial ones that don't have a good model, we can increase our ranks.

The only price we ask is fair attribution to all.

To build the dreamcatcher is to use it.

Multiversal applications - applications designed to operate with multiple conflicting views of reality.

Metaverse as shared imagination anchored in a shared reality.

Effort spent building a universal attribution tree is never wasted - when it eventually unfolds, that effort will be rewarded, as surely the builders of the attribution machine will themselves be attributed to.  As a form of meta attribution.  Meta-attribution is our revenue model - a fee for paying out to everyone worthy.

The app is never in an inconsistent state, and can be recovered from any point.  It is always atomic.

If you can make an unstoppable computer, then maintaining a ledger is trivial.

Chaos model of development.

The Joule as a unit of computational energy.  This can be used as a form of intrinsic value peg for a stable currency that is uncensorable but based on real value.

Blockchain gives data redundancy normalization by default.  Consistency is still the designers problem to solve, but because the chains can be shared easily, consistency between organizations is now possible.

Binary layer is by reference, Action layer is by value.

All the worlds attribution in one hash.  Making a universal attribution table for everything in existence, representing as a single hash.

Micropayments dispersals vs token inflation

Inverted Airdrops - drops to everything ever.

Will the problems of the future be solved by the behaviours of today ?

Removing servers and stock options at the same time seems easier than doing either in isolation.

Services can be guaranteed ethical but goods are a little harder.  Ethical services marketplace lets us access ethical payment for ourselves sooner than full supply chain.

The massively multithreaded world computer.
The hyperthreaded world computer.

The chain itself must not be externalized from the accounting equation - worldchain must have reflection ability, to determine its own carbon impact.

Making a truly public cloud - public to see execution, public provided resources, any language, exotic compute types (GPU, quantum)

The Dreamcatcher goal is to enable fair payments for consumption of open source.  This in itself generates a highly useful dataset.  A distributed application that incentivizes open source developement by attributing payments for consumption.

Weightless economy.  Weightless objects.

Forced attribution - if you don't have force, you don't get attribution.  This is conventional equity.  This is Empire.

A society's attribution model defines that society.  We should be able to model all the historical ones by their attribution models and cite the downfalls as being some form of inequality and inflexibility in their models.  They will have parameters like speed of information disemination, speed of recomposure based on changing data.

The metaverse terraforming the physical world.  Metaverse was always there, it was just disconnected and poorly modelled.

Morality, ethics, fairness, and transparency - decentralized ethics - supression proof platform.  This is why the Dreamcatcher algorithm must operate on the blockchain.  Finality.

The main thing we want to push is the way to find the answer, not any given answer.  As time changes, that answer can change.  Software is like an answer to a poorly asked question at first.

Darinism for contract law - SM

If you are not attributing to everyone, then you cannot complain about being treated unfairly.

Open source requires that derivative code is forced to be open, and that no payment can be forced, but that doesn't mean payment cannot be asked for.  An unenforceable ask.  We are not asking that you pay us, but that you pay all who contributed.  On behalf of all.

Proximity is Power - this is why the island is important.

There is only one DAO - one nervous system - and it has conflicts within it.

If the Packet is given autonomy, then it can find its own way thru the network of innovators.

Remixing of AI inputs seems to be the new coding - must be able to fork and then resteer the AI.

Basically any knowledge based industry not pivoting to an AI based model is doomed.

In addressing for innovation packet routing, source of a packet is the problem statement or spec, destination is the solution.

We always wanted the same thing: Algorithmic Attribution based on Contribution.

Packets turned a transaction into a thing, which allows it to carry so much more information and utility.

No blockchain seems to include the labour pool in their offerings.

AI allowed us to offer more than just feature parity with web2.0

Internet started using phone lines, ethereum started on btc, dreamcatcher started on ethereum - each next network starts within the old one.

The blockchain becomes a product of the network.  Interbit becomes a product of Xinova.

The main feature of the dreamcatcher blockchain is the dreamcatcher protocol that automates equity rewards for contributions of knowledge work.  It can do all the things the other chains can, including bridging between them all, as well as rewarding all contributors both internally and externally.  It does this by way of its ability to AI models natively on chain.

The dreamcatcher EVM contract is basically our company policies - it defines how we act, how we manage work, how we reward our members.

AI is the ultimate need for more compute than you have locally available, and we aim to provide a means of containment to retain independent control of your compute, where the rapid switchability of your compute supplier makes it an ideal market, healing inefficiencies in the cloud computing market.  Access to exquisite hardware like GPU and Quantum, and to effortless scale such as 10,000 cores working for you for 10seconds.

A new interface for applications - natural language - that can be used as the reason for making infrastructure changes of benefit, where the infrastructure change alone is not compelling enough, since users end up with something almost the same.  But the drive to AI means this change has to be done, it is a complete overhaul, and so might as well be done with other longevity improvements.  Also want to contain it, so AI is a natural partner for blockchain.

Provenance is blockchain, self sovereignty is crypto currency.

A self sovereign data platform cannot be made a sovereign provider.
So our offering is to provide an application platform that is self sovereign native, can have LLMs plugged in, 

Starting with smart cities, specifically with medical records.  Building a generalized system means we are not beholden to our specific partners, nor them to us.

Can trust where the grant money went as this is fully traceable.

Data labour and software labour should be treated as the same thing.

attribution, self sovereignty, provenance are what is required for a public health system record system.  Attribution because everyone wants to get paid for their data.  Self sovereign because people want to control their own data.  Provenance so people know it is authentic.

Provenance stops cyberattacks.  

Setting your own pricing policy is the core of the capitalism design flaw - there shouldn't be such a thing as this freedom should be determined by decision automation.

model based testing is declarative testing - say what you want, model figures out how to do it, rather than imperative testing where you arrange things implying what you want

User interfaces are just advice - blockchain makes actual things.  So you use the interface to get advice about what actual things to do.

Humans should be interfacing with humans - thats what they do best.

Standardization comes before automation.

The limitation is that current blockchains creates their value of decentralized security by disconnecting from all other systems, meaning NFT-based assets do not interface with data and systems outside the blockchain (static).  In Dreamcatcher, we include all the external systems inside the blockchain system, providing the decentralized security value with all the liveliness and utility of real world connected systems.  Our NFTs are not inside the blockchain, but are a dedicated blockchain themselves, free to roam the world and ingest information from whatever sources the owners wish.

Confidential information leads to temptation.  Humans can't help by try to overpower their peers with knowledge asymmetries, so having any info confidential in an org will lead to more confidential info over time due to infection, and ultimately will lead to unethical behaviors internally, which lead to demise.

The attribution AI is a Neuron trace for the AI brain

LLMs are the biggest change in UI since the GUI.

Innovation Acceleration

Innovation Acceleration - from dream to reality

Catching Dreams, building reality

The value is not in selling the AI, or the software, but in terraforming businesses using a small coordinated team.  That way, we control the revenue at source, and cannot be outcompeted in the software space from far afield.

Blockchain is required for AI systems that change data, since it gives us ultimate control, rollback ability, provenance.  It gives other good qualities too, but for AI controlled systems, being able to rollback just a small piece of your database is crucial, and being able to fork production for the LLM to test with is critical too.  Fine grained permissioning, at almost the row level, is important as well.  It is crucial for self sovereign data, so that people can have trust that their data is not being misused.  So all comms out of the AI needs to go thru blockchain so it is logged.

"Ok spend it" - lols when the AI warns you what you asked will cost a lot of money to compute but you want it done anyway

Edward Lorenz: "Chaos: When the present determines the future, but the approximate present does not approximately determine the future." - this is the singularity, and once we have enough AI to behave in this way, the immediate job is to dissolve the old structures as rapidly as possible so that all things behave in this way.

And so now we will only consume the internet via an AI proxy

Decentralized AI apps - dAIapps

Backups in a blockchain system give us the ability to take things away and preserve value, rather than store things to preserve value.  So by allowing shortened paths in provenance of the root chain, we can optionally destroy data and free resources.  Backups in a traditional system by contrast are required as a snapshot so we can recover things.  Blockchain lets us recover everything, and do degrading this state for the sake of economy is an inverted way to look at data recovery.

We are decentralizing AI, decentralizing the construction, and the operation of it.  It is continuously constructed, in a decentralized way.

Making git but for 

If pass by value vs pass by reference is content addressed, then there is no difference.  Mutable reference passing is a dangerous thing.  Transclusions in [[Xanadu]] is a debate about pass by reference vs pass by value.

Transclusions is about what changed and what is the context surrounding that change.

The blockchain inside itself means we want to use the state transition model of blockchain to diagnose the blockchain itself, so we can move it forwards and back between determined states to focus in on a given area of concern.  This is essential where the repetition rate of code is high, rather than single code calls, but on the Nth loop, depending on state, the error occurs.  We can publish snapshots of error states and use these to derive more focused tests, and to share with others, and to act as regression tests.  The principle is the state transition principle, which is the heart of blockchain.  It is a cleaner way to work rather that mutating ram then wondering what went wrong.  Reply and undo are essential elements of all our tools for users, so why not for the programs we write ?

Basically a framework for AIs to program with and be programmed interactively with humans.  Like a wikipedia and a stack overflow for AIs to collab with humans on.  The devs we are writing for is AI.  This is good because we can therefore have unlimited users instantly and can quickly know if they are being productive since the users will consume their products increasingly rapidly.

Silly con valley

eco-lux: showing how the eco future need not be miserable, but can be nicer than what we have

We are deploying a universal object memory for global AI. ![[Pasted image 20231219095433.png]]

The dreamcatcher is a decentralized meta LLM that is goaled to increase the innovation rate using fairness.

Packets only paying people out when complete would cause a huge celebration whenever the team landed something - at the moment closing a task is a pretty uneventful thing - only releases are celebrated, if at all.  But it is the tasks that make up the releases, so the celebration should be about them.  So we could have a bunch of people on prem, or in VR, or as a team, and they celebrate when stucks are solved, since they get paid, and the system moves forwards, and they get royalty, and they can trade these units.

The Reckoner - semantic reckoning in an AI world

Hash the world

Git is the protocol of consensus for code.

Work is a moat - if a business can be set up that has a work cost that scales linearly or logarithmically, that is a good business.  The more work you do, the more work anyone else has to do to replace you.  Particularly those with a high initial hurdle that at scale you get more benefits.  This is worth raising for.

Repeatable Computation.  The key to reliable systems with unreliable components is repeatable computation.  This needs to be efficient.  The git model provides a way to to put running application state into git as well as the code that starts the running systems.  Better than logs, put the state in there in its completeness.

It isn't data sovereignty if you can't verify your data.  If you can't compute the integrity of the data set, you don't have data sovereignty.  If your provider doesn't publish attestations about the state of your data, they can change it at whim.  So can an attacker, so an honest provider might not know they failed you.

Theres some kind of separation where the plumbing / transactional internal machinery of a system is different from the payload that it carries.  So using context specific tools to handle a certain context seems fine, so long as they can be swapped out on different platforms.  The key property they enable should be consumed via an interface, and we should enumerate what these properties are, such as consistency, atomic messages, locks, broadcast - these are the primitives of distributed processing - how you get them is context specific, but every distributed system should model these is units.

Repeatability of computation is the ultimate in traceability.

If it isn't repeatable, it isn't secure.

Repeatable computation makes redundancy trivial.

If you want reliable, you need repeatable.

Always besure to use replay instead of absolute reliability of a single system.  Single systems should work in good faith that all their components work as advertised.  Multiple systems are there to ensure no faults in the internal components.

the utility of your data is a function of how many other systems it can exist in. (https://deno.com/blog/webhooks-suck)

Cursed are those who Dream.

The heart of the Dreamcatcher is the ambient attribution system.  From the outset, we viewed the dreamcatcher as an innovation network.  The heart of that network, is the automated fair attribution to all who contribute.

“Right now, there’s this thing where ethics aren’t what they used to be,” Chappelle said. “This idea that people are trying to replace the ideas of good and bad with better or worse, and that is incorrect. You gotta keep your ethics intact, because good and bad is a compass that helps you find the way, and a person that only does what’s better or worse is the easiest type of person to control. They are a mouse in a maze that just finds the cheese, but the one who knows about good and bad will realize he’s in a maze.”