
>[!tip] Created: [2024-04-17 Wed 10:28]

>[!question] Targets: 

>[!danger] Depends: 

We should be able to run standard evals with massive parallelism using our platform.

We can run a live leaderboard, where anyone can pay to rerun the tests, using keys supplied thru the api gateway.

Should use the same systems as what our own evals do, so we can see which model best fits our use cases.  We would also measure cost and latency of the benchmarks as additional dimensions.