
>[!tip] Created: [2025-01-02 Thu 11:21]

>[!question] Targets: 

>[!danger] Depends: 

We need a better voice recorder so that it can recover from what's happening, so if none of the transcription tools work or the API goes down or whatever, at least, or even if we lose net connection, at least the web browser would allow us to have the file, and once we restore connection, we can push the retry button again, which will run it through the Whisper transcription from OpenAI, but also the DeepGram transcription as well. In the meantime, we should have the DeepGram transcription should be running live, so we get some feedback, and we also get left with some text in the event of an error of any kind, and then we'd have a second tab that would represent the Whisper transcription, which is often ongoing. 

Finally, we might want to present a comparison between those two, so we can get a better, more objective handle on the quality of both of them, plus the third tab we want to present, or the third view, is having done the full file through the DeepGram model, just like we did with Whisper, so that all three of them represent different things, and the fourth one might be the browser's real-time transcription API. If we push a button, we can get 01 to do a comparison between all four, and score the relative results of which one is more likely to be accurate, which one's clearer.

Ultimately, the 5th tab, which is the final one, would be a formatted, reasoned, context aware parsing by o1, to try work out what we really meant to say.

Lastly, an o1 reasoned response the turns what we said into a more effective memo, with bullet points and numbers etc, suitable for entry as a command into the wider system.  This is derived from the cleaned up version of what was said.