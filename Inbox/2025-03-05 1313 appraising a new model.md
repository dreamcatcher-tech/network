
>[!tip] Created: [2025-03-05 Wed 13:13]

>[!question] Targets: 

>[!danger] Depends: 

The issue is that whenever a new model comes out, we always need to compare the two results, which can be time-consuming and laborious.

Instead, our platform should do the following: When a new model comes, we run them all in the background along with whatever popular strategies we have. If the first result we got was consistent and inferior, we can check the backups to see if anything's better.

Additionally, the appraisal of whatever the human chooses, we look at the other models and say "well, that was good enough," and therefore we'll call that a pass. 

Additionally, whenever a particular type of question gets asked, we should file that under our own reference for strategies so that we can start to learn general user preferences and individual user preferences. Each time a result comes back, if someone asks something like "how many functions are there in this piece of code", we know that there is already a strategy that we found very popular for doing that and a nice format that we like. We can indicate to the user that we did in fact use this pre-generated formula and that will make the user able to go and modify that formula any time they like to better suit their needs and perhaps the needs of others. 