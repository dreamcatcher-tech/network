
>[!tip] Created: [2023-12-10 Sun 19:51]

>[!question] Targets: 

>[!danger] Depends: 

As soon as any new LLM is released, we should automatically run it thru our evals so we know what the state of things are for our purpose.  Our purpose should be the same regardless of what models are applied against it.

In our test suites, we should be able to automatically update the github source when a better version has been discovered, or at the very least create a PR.

Use GH runners to run on our own hardware with access to GPUs.  We should be able to sell our GPU time to others to run public jobs, particularly those that are deterministic.  We should stake to prove rapidly when someone else has lied.

Ideally we want a release on dreamcatcher.ai that is always our best evaled foot forwards.

We should be having graphs for our evals too.