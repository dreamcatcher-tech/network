
>[!tip] Created: [2024-02-21 Wed 18:20]

>[!question] Targets: 

>[!danger] Depends: 

Seems best if only the changed updates and updated.

Make a base, packed snapshot periodically, that gets stored in s3.

Then keep lightweight low latency copies of only the changed objects which are assembled as layered FS and only as needed, so only when an object is requested is a lower layer requested.

The layers make storage cheap and fast, but there is wasted space and time for more dormant repos, so offloading to s3 saves space and time, but loses latency.

Also makes it very light to do commit watching.

This batching process can just kick off periodically, by using a key with a delivery delay whenever some layers are created, to eventually compress down the layers.

If the layer count gets high, then we should compress early.

If each layer was packed, then the storage of the pack snapshot can be stored in a single kv write, making best use of the db write space.
Periodicially we would pack everything down.

So the pack files could be stored in s3 directly.
So the whole git fs is never a snapshot fs snapshot, but rather is assembled from s3 hosted pack files, and smaller but faster pack files on the local fs.

S3 is good since we can retrieve things using ranges, which means we can lazy load things, like fetch the index of a pack file, then retrieve only the ranges that are being asked for.
Might be cost effective to just grab the whole thing tho.

