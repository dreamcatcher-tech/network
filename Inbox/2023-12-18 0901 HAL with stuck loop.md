
>[!tip] Created: [2023-12-18 Mon 09:01]

>[!question] Targets: 

>[!danger] Depends: 

The stuck loop should be the first thing that gets used, rather than bespoke ways to attempt to integrate apps within apps.  The key is that HAL should be using the stuck loop as the primary tool it uses all the time, and the breadth of search is the only real difference.  How exactly the stuck loop gets entered in to and refined with the user is up for modification but the principle is that it should always be doing the stuck loop.

The problem so far has been that when HAL is not performing well, we try to make a new way of engaging HAL, whereas the stuck loop should allow continual steering, where the only major overhauls are how the stuckloop is engaged, not how HAL solves any particular problem.  This extends to custom bots we make as those bots need constant steering too, and we cannot do it using a single up front prompt - the only up front prompt should be how to engage the stuckloop.

A custom RAG should give lights on when certain stucks are retrieved, so a backtrace (stucktrace) can be generated where we can predictably show how the model responded and what rules or stucks were called upon, so we can see inside the system, and our users can also see how a certain request was fulfilled, which may cause them to change at a stuck solution is.  So they could tweak what help was chosen for a given stuck, so the stuck solution is based on the stuck plus the context, where reputation and also direct user preference is balanced to provide a result.

If the used stucks are shown to the user, then can edit them and reply to them, so we know what was being drawn upon.  We can use user preferences to change what these were, and then average out what a bunch of users said, to affect everyone.  The ranking algo is surfaced to, so you can see why the ranking

Ranked retrieval augmentation, with a focus on problem solving as opposed to knowledge.  Could be used for knowledge since the goal would be "to find something out" and then we would get some help related to this.  Could also find the documents provided by the wider network to solve this particular query.  So even the docs get incentivized and refined using the stuck loop.

A stuck help could suggest installing a particular app to accomplish the task.

HAL need only be prompted how to use the stuck loop and the responses therein.  Executing the helps can be an isolated exercise perhaps ?  The steps are cd, execute, etc, so it might need less context in the thread, as can scope based on the stucks being used.

Chat is just another filesystem path, so when you select a part of the chat, it is just a path change in the interface, and removing a selection takes you back to the previous location you were at.

It could be that different bots end up being layered on top of HAL, where you are talking to that particular expert and the user knows it.

Include the chat history into the stuck loop where it can be chunked down and summarized in some way.  When the stucks are completed, we can just replace those portions with a reference and a summary of what happened, which can be a background thread.  Then when enough of these come together, they can be summarized even further down.  When it gets far enough back, all that is really required is the ability to reply to a particular item to rejog the memory of the bot, to allow that section to be loaded up into context.

In summary, the danger is falling into a conventional software development cycle where we manually try to tune an output rather than making it continually improvable by how the stuck loop makes a dynamic choice based on the best available answers.  It cannot be a dynamic system if we solved it once and only do manual improvement on it.  Wider search should be the same as local search, and it shouldn't be a total paradigm shift to go from local to stuck loop.  Stuck loop should be the first thing that gets selected.

Abandon the coupling between the filesystem and the goaling.  Installed apps influence what helps get selected, as the apps target a collection of stucks, and so if there is conflict, some clarification is needed from the user.  We can use the CD as a way to scope an favour results, but they are not directly related to goals / stucks, they are 

Helps may be generated by AI where the AI just reads the docs of all the available function descriptions, runs some tests where it tries to make the changes it wants in a sandbox, and then condenses down the results.  So what to do is done in faq style, and the app is provided and refined using descriptions which can be tweaked to match the most used faq paths.

So a bot then is really some base prompts for tone and style, then a search restraint on which stucks it can deal with.  Stucks and helps act as the examples section in the prompt, where they say what can be done, as they are inserted as examples into the prompt.  If the helps rank similarly, they are inserted with their score, where the score is simply the distance that their goal was from the input goal.  The quality of the helps can be improved by humans, and assessed by bots so that improvement is incentivized and open.  Means that the bot base prompt often stays short and simple, but the example quality goes up continually, and expands with more examples to draw upon.

The ranking of the strength between the stucks and the helps is continually assessed in the background, and each consumption feeds back a relevance.

So the tooling to be able to cherry pick the retrievals that were pulled up should be available to the users as a form of submitting helps.
Help may be actually a redirection to existing solutions, or a clarification process for the AI to run to transform the question such as be able to fit a template, or call some code that then transforms the problem.
Multiple people with conflicting solutions should be averaged out some how, and this averaging behavior expressed to the consumer.

We might run the user query directly thru the stuckloop to see if we get a strong enough hit.  Only if it fails to hit, do we use AI to modify it so it gets stronger hits.

## Admin tools
So admins using our tools to solve their own stucks from their bot can also get stuck when they try do things that would filter up to us.  They are hitting the edges of the box and we are adapting, just like their users are hitting the edges.  Goalspace is flat, so even their users can still trigger our actions if they get stuck in the right places.
## Talking to AINodes
The distinction is not as strong any more, with a relaxed binding to the artifact layer.  

## Goal breakdown
This is only needed to improve the performance of the query.
We might store goals in order to come back to them when stucks are solved, or to show a list of completed ones as a way to measure goalrate.

## Fundamental Feedback
The fundamental process is to take the user input and retrieve a list of instructions that can be used to complete the task.  

Each time we ultimately solve a stuck, we should include a genericized version to be published so we can solve that again.  
So the goal of the goalie is simply to genericize / clean the data, then submit the stuck, then update the response once it thinks it was solved.  Detecting when solved is important, as this is the assessor function.

This takes the huge load off us to get the system right, and it is really an ever growing training set. We leverage that the user actually wants to get their task completed - they are not malicious.

## Engaging the stuckloop
When we get back low confidence retrievals, then we should flick over to the stuckbot, which is an expert at getting out a good quality stuck.  If during clarification a suitable solution is found, it will flick back to the applicator, which is an expert at following the directions from the helps.

## Phasing
1. stuck loop with manually generated helps to start
2. helps filled in from app descriptions

## Techniques

### Go back to the old api
We can do our own retrievals, where we break down the user request into goals, and then use our own retrievals to figure out what context to insert.  The dreamcatcher is an open context tool

### Fine tune on stucks
Use stucks plus help files to generate fine tuned models so that when a certain stuck is detected, the model chooses the paths we have included in the training, allowing for a limitless number of examples to be provided.

### Use the retrieval tool
Problem is that we have to rely on this tool working when we cannot see inside of it.  Its also hard to run parallel calls using the assistants API, whereas the old api we can do multiple concurrent calls easily.  Plus we can force function to be called or not.  

### Make our own vector search
Whilst slower to start, this could be more long term fulfilling since we need to be in control of this piece anyway.  Once the tool is invoked, it gives immediate answers but begins wider search activity in case it gets asked again.

## V1
User: (asks whatever the user does)
HAL: breaks down into goals that are used to search, and calls the search function
Tool: getHelps( goal1 ) => a series of helps that are returned
HAL: (gets called with the help, and the )

loop around until all the tasks are met, altho tasking is a separate thing ?

gets the stucks, gets the helps, then executes the calls.  These could be truncated threads, since we can scope the request down greatly.
Ultimately this would result in 