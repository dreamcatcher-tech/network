
>[!tip] Created: [2025-03-13 Thu 21:40]

>[!question] Targets: 

>[!danger] Depends: 

After the fact, we could look at our responses and imagine how they could better adhere to the rules that the user had stated. What we can then do is engage in direct preference optimization or other types of fine-tuning to continually integrate these rules using the new use cases we have seen and our perceived shortcomings in them.

This means that when you set a rule, it will be manually adhered to, and we may correct ourselves during the first little bit. Given a few cycles of tuning, the model will become more preferable to you based on these rules. We can also now generate synthetic data once we see real data come in.

we could very much tune between people in an org, between people in a country, or other types of psychometric joins. 