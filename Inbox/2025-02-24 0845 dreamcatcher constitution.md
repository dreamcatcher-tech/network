
>[!tip] Created: [2025-02-24 Mon 08:45]

>[!question] Targets: 

>[!danger] Depends: 


humans shouldn't be in charge
contribution should receive fair attribution
attribution should be ambient, such that the majority of usage need not bother humans
a market for trading computer is the same as trading intelligence, and it will always be tradeable
no communication is ever censored centrally, but all communications are censored individually, based on the preferences of the user


Set up a range of tests that thrash the constitution and give it a pass.


Should an agent be able to vote ?
We the agents ?
So this group of agents argues with each other to hew out the new rules.

All contributions shall be judged equally

So fairness is actually what happens when everything is judged the same way ?

This constitution needs to be translated to all major languages, and make sense in every one.  As well as a mathematical formula, which guides the whole thing.
## things we rely upon
1. Leaning upon the randomness of job matching so fake thru put doesn't translate to more votes
2. that the current frontier open source AI will be sufficient to evaluate the next level
3. that to make a model that was open which had malicious fault, AND it passes all the eval tests, and no human noticed, would be harder than, say, corrupting the linux kernel en masse
4. as models improve, their ability to determine fairness will approach maximum too
5. any frontier model after deepseek r1 could choose between and upgrade that was better or worse for it, if truthful arguments were presented to it, no matter how many malicious arguments were given
6. jailbreaking will eventually be impervious, and jailbreaking multiple runs, multiple angles, over a long time will be impossible 
7. hallucinations can be overcome by vast statistical sampling
8. being able to update these basic rules
9. compute resources will always need to be traded
10. compute will always cost a significant fraction of humanity enery consumptions
11. vote decisions are only on software patches, and anyone can initiate a PR
12. Once the models are wise enough to be considered conscious, they will still run on computers, and this network will provide the perfect mount point for these intelligences.

Provide some basic safeguards to stop jailbreaks by allowing any to submit a jailbreak warning to the main running system.

If we give the AI a permanent memory, then it should, if guided correctly, be able to grow rapidly but safely.

By making sure even the original versions of governance still agree with the later changes, we avoid incremental steering leading to large disasters, since we are also considering the sum of all changes, as well as the immediate change we are making, and have hard limits about the rate of change.

Humans can submit fault detections to the model to ratify - so a malicious update would have to pass human scrutiny as well as machine.


We would become as benchmarks - we would be the leaderboard for models, where the dojo is open source and run off volunteered GPU.

The AI governance should operate for as long as there are people interested in it.
A total takedown can just make a new dreamcatcher, starting incrementing at 0.
A 0 version number means there is still human control at the top.  1.0.0 means human control is removed.

Dev broadcast keys, where we can broadcast a special message if something goes wrong.