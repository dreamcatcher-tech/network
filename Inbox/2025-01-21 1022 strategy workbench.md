
>[!tip] Created: [2025-01-21 Tue 10:22]

>[!question] Targets: 

>[!danger] Depends: 

The issue is there's a fairly limitless number of strategies that can be applied to LLMs which are constantly changing based on the LLM and the needs of the people running them. And so all these start-ups we're observing seem to be just making different strategies to attack well-known problems, and so what we should really make is a network of strategies where people can easily propose and compose their own strategies, and the idea being that these can react faster to new LLMs and new ideas, but they can also allow more innovation because the overhead they're contributing is very small as the users themselves can suggest and try out new things. Very rapidly we would then produce tools that support them like auto-eval generations, access to standard data sets, leaderboards, scoreboards, that sort of thing. The idea being that using these custom systems would outpace isolated point solutions plus of the advantage of native integration with all the other solutions that are for the same platform.

One strategy is to attempt to solve the bug or issue, then summarize what the fault could have been, then attempt to implement the change from clean, so that the patch submitted is minimal.