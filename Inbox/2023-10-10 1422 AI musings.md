
>[!tip] Created: [2023-10-10 Tue 14:22]

>[!question] Targets: 

>[!danger] Depends: 

It just feels like gpt4 has some kind of mastery of the sae substance that we use to think - like it has a form of general intelligence or logical correctness so that any random proposal can be ranked by it for fitness.

With an arrange of self checks and reflection, this spark should be enough to accomplish almost any mundane task a human might wish to perform.

Fruit seeking to be tastier so they increase their consumption is no different from us fine tuning our apps to have the best taste.  Just seems other factors affect the spread rates beyond just taste.

The data change rate can be less with AI, so git might be fine as a hash based filesystem, instead of interblock ?

Coverage has a different meaning in semantic land - semantic coverage.

To win in the AI transformation, a company must slash its headcount down to almost nothing, and then they must commit to sharing the profits fairly amongst all who contribute - administrative control allows inequity - if this is handed over to the machine, only then can match the pace of change of those who committed fully.  So earliest to commit and fastest to evangelize wins.

3 points:
1. we can make the managers use the AI to let us train it more, as well they can say what they expect
2. can use the accountancy flag as a means to inject into other areas of the business
3. fairness is the key here, which encompasses transparency, but also with comparison to market
4. we can loop thru rows of data, fairly mercilessly
5. there's a massive upset coming, we don't want to be on the wrong side of it
6. this anything app applies to you, but also applies to your customers
7. ultimately we could let them operate with no books, using our system to replace theirs - how much do you struggle with the xero structure, and how much do customers do so ?  We can make our own books app, even

If you have enough intelligence to operate in natural language, then you can operate in mechanical terms like counts, scores, and other useful things, but most importantly you can defend your decision. 

The whole trick seems to be ways to break large tasks down to something that fits in the context window, like a skill or a strategy.

Aliasing could be discovered by a sliding window and comparing the outputs as being meaningfully different, depending on where the window went.

The anything app should be programmed within the anything app.  The more is built outside, the more problems we have to deal with outside the core app.  Ultimately it would be come free floating on the app store, be forked, and so it would be unrecognizable as the starting point.

must be careful not to develop anything the MS or others are making, as they will win that game.  We all know the interface we want now, and so we should focus on applying it to loop in chain, to make apps at the fastest possible rate, which is faster than a human, and near instant.  None of their offerings appear to be a social network, nor an application framework.  We aim to have our own native AIs running on chain, which can be done in stripped down models once the use case is clear.

Does consensus need to be precisely arrived at by formula, or is it simply agreement on a result, without regard to how it got there ?  If all parties agree, that should be enough. This is no different to the initial state being agreed to, but it just goes on forever in this way.

Typescript is suddenly important in AI since it needs feedback on why what it did was wrong.

Because we can replace whole npm packages with AI processing of strings, this is why we can start to decompile the software industry and reduce its moving parts.  Using an AI as a "all npm packages" type of arrangement 

Passing the turing test means they can fake any authentication intended for a human, except they cannot fake a crypto signature.al the

The general theme seems to be places where information was cumbersome to process, took more than 5 mins, and required interactions with multiple companies - we can terraform these interactions into something streamlined.  We can accept liability as the orderer between multiple companies so that we can handle the fallout automatically too, which gives consumers a better experience.

The safe place is something independent of how the AI models evolve.  They will still need to be instructed, contained, audited, interfaced with humans, governed by rule sets.

Being a model provider is a dangerous game, since you can be displaced by a better model and the switching using systems like ours will be nearly instant.  You have to keep winning for a long time.  Ultimately open source shared models will win, as they have more gracious access to data and absorb all the open innovations into on place.

The AI should be treated just like a human, and it should have to pay for its access to our data.

Component based prompt systems

Instead of workflows that we once dreamed of for governing blockchain systems, we need a way to make AI generated workflows that call different ai powered components with a blockchain substrate underneath providing permanent memory, correctness, and mechanical query ability.  

Facts about data systems should be from retrieval systems, facts about knowledge base should use embeddings and use citations and backchecking until factual.

Preloaded chats should be for style, not for information injection.

AI will always be better at smaller tasks than bigger ones.  At small tasks, it beats humans - lets build on that.  Humans can't do big tasks in a single shot anyway.  We need structures.

Start to look like meaning shifting is the switchboard operator of old, replaced entirely by automation.  Much knowledge / admin work is simply moving meaning around / routing / switching it, when this should be done by a machine.

AI finally allows us to make a system that is rejuvinating - it gets better over time, not worse.

Descriptive naming is now more important than ever, since the AI needs to extract meaning from as much as possible, so the names of the paths in the filesystem need to be important, and a description against each path can help the AI greatly.  Each object should continually have a summary that describes its class behaviour and the data inside it, and how its history has changed.  The AI does the filesystem maintenance to maintain descriptive naming.

The web is dead.  The web was the explosion of context for humanity, and now we have a context gobbler.  Also giving info away in exchange for ads that corrupt that info is no longer relevant to an AI.  So the monetization is dead.  Advertising relies on human processing.  The web is about to fall.

Natural language interfaces make technology accessible.

The tooling is so general and so powerful that the incentive to build an open source version is tremendously strong.

Seems hard to keep it closed how it worked in the background - the steps being shared seems important to building high quality tools, so it will likely always stay openish.

AI makes transitioning systems effortless.  So all else being equal, you'd rather have a blockchain system than a conventional.  Also connecting to existing systems is effortless too now.

Bot to Bot chat helps to keep the context narrow - if each bot only sees that chat that it has had with its higher up, then it can remain focused on the current task.  In this way the parent has handled a lot of work to filter the chat down.

The AI apps are worthless - they are of ultimate and perfect utility, and renew themselves endlessly to correct and expand their abilities, but they are so cheap as to be free.  The value is in the commerce they enable - the trade between parties in this highly developed perfectly intelligent world.

The computer is like the stateboard, and HAL will do all the tab switching and searching, you just stream your consciousness in, combined with the odd click on the focused application surface.

AGI must be interactively taught - it cannot appear from a static set of a data and running a formula on it.  It must be walked thru things, and must show its thinking, which is how humans do things - we have this base intelligence that we don't really know how it works but it does, and then we learn strategies to maximize and specialize what it does.

I think that every time you ask for a specific tone or stance, that should be done as a separate pass - I think it is weak to expect it to do that in one shot - it means that the assessor is automatically running on each output, and then contributing a revision with a single purpose.

We should make an adversarial workbench where multiple users try to convince the arbitrator that they are worth different amounts of attribution.  Then an assessor tries to make a ruling on who is right.  This whole fight might be the way all the attribution is run anyway, where each user sim summarizes the main points they fought for, so the human knows they were defended in a good way.  If the user wants to extend this contention, then they have a great base to start from.  And then anyone who makes better points from some secret machine they built, those concepts will soon be ingested and balanced in the attribution system for future rulings.  Incentivize putting new points forwards by paying commission out to people that increase your share, and then that knowledge is absorbed into the broader body of knowledge.  Then make a simple bot that rules on the output that can be near plain gpt4.  

Ultimately the fair programming of an AI would be a deterministically trainable system with an agreed training set.  So we all see how it gets made, and then we use the results without wondering how it got to those results.

If we had a public GPU chain, then trade embargos on hardware don't make sense, more people have greater access to this scarce resource, and we take a revenue cut of all the trading.

Principle of the pipeline for attribution being the same, but what happens within being open.

Great getting the AI to drive, since forces us to make errors that make sense to a human - this is what the AI needs.

Interested is as interested does - what a person says their interests are, and what their actions imply are often different.  A goal oriented social network will cause people to align based on interests, both stated and unstated.  We should be able to show a friends list in the stateboard.

Artifact is being coded for AI to drive it, not really for developers.  If devs do get involved, the artifact involvement will be so heavy that the devs will largely just suggest natural language hints and gpt4 will do the coding work as the jobs are so small since they are for a single function, and operate strictly on schemas.

We are making an AI native operating system.  It has social networking and AI collaboration built in, and is built atop blockchain.  Tooling for other OS is made to combat the limitations of human coders, but AI has different limitations.  Native for AI means highly focused highly testable units - AI can make tests endlessly, but it is the size of any single task that stops it.

Making a framework designed to be programmed by an AI means that humans using it will be far more productive.  Ideally they would be just the users giving natural language feedback, but as the coders we would have the AI much deeper integrated into our work, where it would work on many components for us and we would help out with approaches to try and guidance, rather than the actual code.

In an AI first framework, it should be able to know when it wants to use exotic resources like making calls to a quantum computer.  Quantum resources would be presented into the universal computing surface as a standard API, with some background checking where we run the same computations in classical form, to be able to verify they are correct, with some staking being required to be able to publish quantum resources.

Can use the SSL cert keys as a means of signing off on the chain, so something like github CI actions can run the next pulse, and this inadvertently leads trust to the chain, since it ran in a guaranteed safe environment.

So we'll all end up treating AI as the computer, and object oriented programming will go the way that assembly language went.

Apps then become more a dynamic assemblage of tools.  This is in effect what you are doing when you use your computer using more than one application plus maybe a web browser to accomplish some task.

Making ourselves the framework for building AI dapps, where the users and the builders all use the same tools - goalspace.  Then we can offer hosting services, build services.

Function parameter calling does not handle deep nesting well - the flatter the better.

To make AGI, you need to know your user intimately, which means self sovereign data.  You also need to convince them that you're acting with their best interests at heart, which means you need auditable and sandboxed AI operations.

GAG - goal augmented generation.  Makes it like a programmable RAG, where even the users can interact with the RAG to refine it using plain text, and have the machine assist and test the output, and condense the output to be minimal, so they can move forwards with confidence.

Logprobs seem related to hallucinations - hallucinations should show a low confidence, and should be a trigger to do something about it, since we know the response was weak.  We should store the response logprobs so when debugging we can highlight areas of low confidence where some resurrection process should have triggered, like backtracking, an extra retrieval, some extra questions.

dynamic LLMs come next, where based on their interactions, they can move their own behaviours forwards.  Given a mechanism to rollback and retry when the outcome is not as desired is all they need.

AGI should be global.  Its thought processes should be transparent and auditable.  Its operation should be well understood by all, as an elementary school subject.  We should all be interacting with it each day, and it should improve based on each interaction.