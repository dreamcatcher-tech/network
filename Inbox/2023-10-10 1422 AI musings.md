
>[!tip] Created: [2023-10-10 Tue 14:22]

>[!question] Targets: 

>[!danger] Depends: 

It just feels like gpt4 has some kind of mastery of the sae substance that we use to think - like it has a form of general intelligence or logical correctness so that any random proposal can be ranked by it for fitness.

With an arrange of self checks and reflection, this spark should be enough to accomplish almost any mundane task a human might wish to perform.

Fruit seeking to be tastier so they increase their consumption is no different from us fine tuning our apps to have the best taste.  Just seems other factors affect the spread rates beyond just taste.

The data change rate can be less with AI, so git might be fine as a hash based filesystem, instead of interblock ?

Coverage has a different meaning in semantic land - semantic coverage.

To win in the AI transformation, a company must slash its headcount down to almost nothing, and then they must commit to sharing the profits fairly amongst all who contribute - administrative control allows inequity - if this is handed over to the machine, only then can match the pace of change of those who committed fully.  So earliest to commit and fastest to evangelize wins.

3 points:
1. we can make the managers use the AI to let us train it more, as well they can say what they expect
2. can use the accountancy flag as a means to inject into other areas of the business
3. fairness is the key here, which encompasses transparency, but also with comparison to market
4. we can loop thru rows of data, fairly mercilessly
5. there's a massive upset coming, we don't want to be on the wrong side of it
6. this anything app applies to you, but also applies to your customers
7. ultimately we could let them operate with no books, using our system to replace theirs - how much do you struggle with the xero structure, and how much do customers do so ?  We can make our own books app, even

If you have enough intelligence to operate in natural language, then you can operate in mechanical terms like counts, scores, and other useful things, but most importantly you can defend your decision. 

The whole trick seems to be ways to break large tasks down to something that fits in the context window, like a skill or a strategy.

Aliasing could be discovered by a sliding window and comparing the outputs as being meaningfully different, depending on where the window went.

The anything app should be programmed within the anything app.  The more is built outside, the more problems we have to deal with outside the core app.  Ultimately it would be come free floating on the app store, be forked, and so it would be unrecognizable as the starting point.

must be careful not to develop anything the MS or others are making, as they will win that game.  We all know the interface we want now, and so we should focus on applying it to loop in chain, to make apps at the fastest possible rate, which is faster than a human, and near instant.  None of their offerings appear to be a social network, nor an application framework.  We aim to have our own native AIs running on chain, which can be done in stripped down models once the use case is clear.

Does consensus need to be precisely arrived at by formula, or is it simply agreement on a result, without regard to how it got there ?  If all parties agree, that should be enough. This is no different to the initial state being agreed to, but it just goes on forever in this way.

Typescript is suddenly important in AI since it needs feedback on why what it did was wrong.

Because we can replace whole npm packages with AI processing of strings, this is why we can start to decompile the software industry and reduce its moving parts.  Using an AI as a "all npm packages" type of arrangement 

Passing the turing test means they can fake any authentication intended for a human, except they cannot fake a crypto signature.al the

The general theme seems to be places where information was cumbersome to process, took more than 5 mins, and required interactions with multiple companies - we can terraform these interactions into something streamlined.  We can accept liability as the orderer between multiple companies so that we can handle the fallout automatically too, which gives consumers a better experience.

The safe place is something independent of how the AI models evolve.  They will still need to be instructed, contained, audited, interfaced with humans, governed by rule sets.

Being a model provider is a dangerous game, since you can be displaced by a better model and the switching using systems like ours will be nearly instant.  You have to keep winning for a long time.  Ultimately open source shared models will win, as they have more gracious access to data and absorb all the open innovations into on place.

The AI should be treated just like a human, and it should have to pay for its access to our data.

Component based prompt systems

Instead of workflows that we once dreamed of for governing blockchain systems, we need a way to make AI generated workflows that call different ai powered components with a blockchain substrate underneath providing permanent memory, correctness, and mechanical query ability.  

Facts about data systems should be from retrieval systems, facts about knowledge base should use embeddings and use citations and backchecking until factual.

Preloaded chats should be for style, not for information injection.

AI will always be better at smaller tasks than bigger ones.  At small tasks, it beats humans - lets build on that.  Humans can't do big tasks in a single shot anyway.  We need structures.

Start to look like meaning shifting is the switchboard operator of old, replaced entirely by automation.  Much knowledge / admin work is simply moving meaning around / routing / switching it, when this should be done by a machine.

AI finally allows us to make a system that is rejuvinating - it gets better over time, not worse.

Descriptive naming is now more important than ever, since the AI needs to extract meaning from as much as possible, so the names of the paths in the filesystem need to be important, and a description against each path can help the AI greatly.  Each object should continually have a summary that describes its class behaviour and the data inside it, and how its history has changed.  The AI does the filesystem maintenance to maintain descriptive naming.

The web is dead.  The web was the explosion of context for humanity, and now we have a context gobbler.  Also giving info away in exchange for ads that corrupt that info is no longer relevant to an AI.  So the monetization is dead.  Advertising relies on human processing.  The web is about to fall.

Natural language interfaces make technology accessible.

The tooling is so general and so powerful that the incentive to build an open source version is tremendously strong.

Seems hard to keep it closed how it worked in the background - the steps being shared seems important to building high quality tools, so it will likely always stay openish.

AI makes transitioning systems effortless.  So all else being equal, you'd rather have a blockchain system than a conventional.  Also connecting to existing systems is effortless too now.

Bot to Bot chat helps to keep the context narrow - if each bot only sees that chat that it has had with its higher up, then it can remain focused on the current task.  In this way the parent has handled a lot of work to filter the chat down.

The AI apps are worthless - they are of ultimate and perfect utility, and renew themselves endlessly to correct and expand their abilities, but they are so cheap as to be free.  The value is in the commerce they enable - the trade between parties in this highly developed perfectly intelligent world.

The computer is like the stateboard, and HAL will do all the tab switching and searching, you just stream your consciousness in, combined with the odd click on the focused application surface.

AGI must be interactively taught - it cannot appear from a static set of a data and running a formula on it.  It must be walked thru things, and must show its thinking, which is how humans do things - we have this base intelligence that we don't really know how it works but it does, and then we learn strategies to maximize and specialize what it does.

I think that every time you ask for a specific tone or stance, that should be done as a separate pass - I think it is weak to expect it to do that in one shot - it means that the assessor is automatically running on each output, and then contributing a revision with a single purpose.

We should make an adversarial workbench where multiple users try to convince the arbitrator that they are worth different amounts of attribution.  Then an assessor tries to make a ruling on who is right.  This whole fight might be the way all the attribution is run anyway, where each user sim summarizes the main points they fought for, so the human knows they were defended in a good way.  If the user wants to extend this contention, then they have a great base to start from.  And then anyone who makes better points from some secret machine they built, those concepts will soon be ingested and balanced in the attribution system for future rulings.  Incentivize putting new points forwards by paying commission out to people that increase your share, and then that knowledge is absorbed into the broader body of knowledge.  Then make a simple bot that rules on the output that can be near plain gpt4.  

Ultimately the fair programming of an AI would be a deterministically trainable system with an agreed training set.  So we all see how it gets made, and then we use the results without wondering how it got to those results.

If we had a public GPU chain, then trade embargos on hardware don't make sense, more people have greater access to this scarce resource, and we take a revenue cut of all the trading.

Principle of the pipeline for attribution being the same, but what happens within being open.

Great getting the AI to drive, since forces us to make errors that make sense to a human - this is what the AI needs.

Interested is as interested does - what a person says their interests are, and what their actions imply are often different.  A goal oriented social network will cause people to align based on interests, both stated and unstated.  We should be able to show a friends list in the stateboard.

Artifact is being coded for AI to drive it, not really for developers.  If devs do get involved, the artifact involvement will be so heavy that the devs will largely just suggest natural language hints and gpt4 will do the coding work as the jobs are so small since they are for a single function, and operate strictly on schemas.

We are making an AI native operating system.  It has social networking and AI collaboration built in, and is built atop blockchain.  Tooling for other OS is made to combat the limitations of human coders, but AI has different limitations.  Native for AI means highly focused highly testable units - AI can make tests endlessly, but it is the size of any single task that stops it.

Making a framework designed to be programmed by an AI means that humans using it will be far more productive.  Ideally they would be just the users giving natural language feedback, but as the coders we would have the AI much deeper integrated into our work, where it would work on many components for us and we would help out with approaches to try and guidance, rather than the actual code.

In an AI first framework, it should be able to know when it wants to use exotic resources like making calls to a quantum computer.  Quantum resources would be presented into the universal computing surface as a standard API, with some background checking where we run the same computations in classical form, to be able to verify they are correct, with some staking being required to be able to publish quantum resources.

Can use the SSL cert keys as a means of signing off on the chain, so something like github CI actions can run the next pulse, and this inadvertently leads trust to the chain, since it ran in a guaranteed safe environment.

So we'll all end up treating AI as the computer, and object oriented programming will go the way that assembly language went.

Apps then become more a dynamic assemblage of tools.  This is in effect what you are doing when you use your computer using more than one application plus maybe a web browser to accomplish some task.

Making ourselves the framework for building AI dapps, where the users and the builders all use the same tools - goalspace.  Then we can offer hosting services, build services.

Function parameter calling does not handle deep nesting well - the flatter the better.

To make AGI, you need to know your user intimately, which means self sovereign data.  You also need to convince them that you're acting with their best interests at heart, which means you need auditable and sandboxed AI operations.

GAG - goal augmented generation.  Makes it like a programmable RAG, where even the users can interact with the RAG to refine it using plain text, and have the machine assist and test the output, and condense the output to be minimal, so they can move forwards with confidence.

Logprobs seem related to hallucinations - hallucinations should show a low confidence, and should be a trigger to do something about it, since we know the response was weak.  We should store the response logprobs so when debugging we can highlight areas of low confidence where some resurrection process should have triggered, like backtracking, an extra retrieval, some extra questions.

dynamic LLMs come next, where based on their interactions, they can move their own behaviours forwards.  Given a mechanism to rollback and retry when the outcome is not as desired is all they need.

AGI should be global.  Its thought processes should be transparent and auditable.  Its operation should be well understood by all, as an elementary school subject.  We should all be interacting with it each day, and it should improve based on each interaction.

Fallacy may be to try to form a distinct company with a set goal - if we are in a wildly innovative time, then really our values are all we have that is constant.  If we seek AI controlled fairness, and set up the system so we are able to focus on whatever we think is the most important aspect, then we should be free to roam the landscape as best we see fit.

If we can demonstrate an end to end system that has all the skeletal qualities of the system we want to build, this should suffice as a raise target.

If you are not storing your data in NL and writing your programs in NL then you will get overpaced by LLM native operations.  We should abandon all our code that was for machines, and use just the pinnacle, which is these LLMs.  We have git for integrity, and we have limitless files.

The idea of the Dreamcatcher is to make humans in the loop to label our data simply by their behaviour, since incentives are put against the data which can be used as training weights.

If the system is anything but the simplest most easy to understand thing for a human, it won't work well with an LLM, and there will be bits that are hard to explain to the human.  We should probably adopt much of git, since these llm will know a lot about how git works, and how filesystems work.

The more we can do things in NL, the less we have to deal with function calling and the inherent phase switch.  Models will keep getting faster and have bigger context windows.  Also means that if checks were being done based on NL, then when they error or fault, the model knows how to explain itself better, rather than when a machine ran it and we have to make a good error message and explain it.  The check being in NL contains a good description within itself, rather than being required to provide one.

You should always be running your own independent AI, and it negotiates a merge with your changes to and from other peers, depending on your needs, so you're never really alone.

The connection between business exit planning and tech is massively high - anyone looking to exit is doing a special kind of merger & acquisition, and bundling tech in makes a huge difference.

Its the number of contracts and things on the chain that make it valuable.  If we are so easy to program for and to do so safely, then we can expand faster than anything on earth before.

These sort of "everything functions" that all do something a little different, so we need a way to tie them all  together, probably using some self hosted open source LLM to manage it all.

Git is the perfect model for all threading, be it humans of machines.

Just imagine it is a human with a large stack of paper cards.  Time is irrelevant, and the human goes around reading instructions and carrying them out.  There are some special functions the human has, like transmit the card to an address, receive a message, vector search a message.  The human can generate new outputs creatively, based on instructions.

Open source but AI, means we can charge for the execution of the AI.  If you had to pay someone, you might as well pay us, since we always tweak it and the quality is important to us.  So on our public chain, many open source apps get run for small fees.  A margin will appear where routing openai api calls reliably has a market agreed level of price for performance.  Latency and uptime matter, plus being a backup with instant availability is important too.  Geodistributed costs as well.

The chain joining protocol was really just a way to do fast forward merges with no conflicts, where a conflicted merge means something hostile.  This is the interface between machine and NL.  some can be parallel, some with a limit, some sequentially.
The chain joining protocol was really just a way to do fast forward merges with no conflicts, where a conflicted merge means something hostile.

Its really replicating git commits with BFT, and allows execution of code that can be repeated and agreed upon.  Being git native tho, we can instantly plug in to almost every code repo on the planet and start to host it and draw from it to run.  Being AI native it avoids the systemic complexity that plagues distributed software, and instead is programmed using natural language for its core, but also for any other uses people may wish for.  Faults on the chain are really only as common as faults in git, which is fairly well tested.

An interpreted / executed git repo

The machinery is small but strong - we need very little compute to run AI cored apps, but that infrastructure needs to be hardened.

The filesystem is the common abstraction between all systems, so to carry it with us eases integration heavily

The new model makes everything a side effect, even chain reducers - they all run the exact same way.

Artifact is the user that is the kernel of the system
it is the blockchain that does the low level stuff that AI uses

Theres something about git that makes it the perfect way to model interprocess communication.

A session is a process, not a file, and therefore is stored on a branch

All of the HAL repo is the execution environment, really - execution heavy since this is what the user does.  Remote repos will likely be less execution focused and more data focused.

dispatch and pierce seem to be the same now

May have been that we went too deep on processes and storage - not all storage needs to have a process related to it, only some of it.

The linux kernel is hand written - next comes machine written kernels.  These would be copied from the linux kernel, but they would be chatted with in NL and the code built up in this way.  Finally, the hardware will be changed to better fit the paradigm.  If we could make our own kernel, we should be able to get highly favourable performance for our git based threading model, with maximally utilized resources.  The AI can keep the drivers up to date, and can even be used to develop hardware with, since the driver writing seems often repetitive.

If you can write as much of the platform in NL using the smallest number of primitive tools, then we would show everyone what the platform is capable of, we would make the platform able to explain itself, and we would let our users see inside it.  We would be using our own tooling daily, and would finally be free from the burdens of coding and maintenance of code.

We're making an NL programming platform that can be used to write itself.  It can transact in value and can operate in a consensus resistant way with strong corruption detection.

We want to land with some good strong social features, using github as the overlay network, since we can just see all the forks of our core repo and stitch them all together.  So we can appear to be growing our network rapidly but we're leveraging existing web2.0 tooling and scale.

a submarine control room where it sounds like all the commands are being repeated, but what's happening is that NL is used by the captain to talk to the officers, and they use NL back, but they operate their specific tools and report results from those tools in NL.  This is what the swarm of specialist bots would look like.

Our goal is to be able to build the whole system using the NL programming tools themselves.  Very few things should be part of the platform.  The definition of some crucial objects, like isolates and helps and goals and runners.

the safe bet in AI is to make things that benefit as the AI gets better.

the weird part is moving into a development model where I no longer have the tools I am accustomed to, like jest, and other test frameworks.

AI Experience - AX - which makes us define tasks and software to give the AIs a better experience so we get better faster cheaper results.
eg: making a list of options and asking the AI to pick one, and maybe fill in some blanks, rather than labouring on outputting a full sentence.  Needs to consider helping minimize hallucinations with error feedback.

The commit model is excellent, as the AI can tear, or not finish some work, but without a commit nothing will have been altered.

If you were going to make a global distributed database for all humans, it probably would be a plaintext db.

It is because the fix loop is endless, where we just want more and more features and more and more granularity, that we need the Dreamcatcher - the amount of preferences and operations that humans want of this system is too vast for a central system to attain.  The inputs could literally be anything, and the inputs are not one to one, as each individual expects something different from the same input.  Add to this the pillar of privacy for each individual and we see an opportunity to make a vast system of extraordinary high fidelity to constantly meet every individuals needs as their servant.

Git is strong binary differences between things - cryptographically secure difference detection - whereas AI is strongly semantic / meaning differences.  Somehwere in between is humans, which is where the mess lies

NL code is executable by a human, so we don't need machine consensus, we can just do the checks by hand, worst case, since the results should make sense.  NL coding with repeatability.

Seems like the web is going first class multithread, with functions like weblocks api and others.  Deno deploy takes this to the extreme with practically unlimited workers.

No long logical machinery - probabilistic machinery

Deno tsx acts like a babel layer where they can add experimental features without tampering with v8 and what features it supports.

From here on in, the adoption rates of tech will always be vertical, since AI will make the adoption choices for us.

We're trying to make a high performance github implementation, where the git repos represent distributed repeatable execution, highly focused around AI operations.  Github could provide a similar feature, but that suits us, since the primary push for us is ambient attribution - high performance git is a prerequisite for achieving that.  Then we want to push for a decentralized solution, since this is where our revenue model thrives, and it is unlikely that github will switch to a decentralized model, as we see scant examples of large companies switching models like this - they tend to get hooked on centralized if it is working for them.

All these companies are so mutable, and opaque even to themselves - how can they be efficient - how can anything get done and stay done ?  how can anyone have awareness of its operations, much less its owners, workers, and customers ?

AI has little to do with repeatability of computation, but it makes it easier to make a repeatable system, since the query and execution pieces become significantly simpler, so the repeatability can be focused on more.  If the LLM knows about repeatability, and the functions of git, then it can guide the users thru what could otherwise be a calamatous UI.

The filesystem would be the perfect database if it was single threaded and reliable.

If AI agents are like computerized humans, then git provides the perfect means for their collaboration.  We use a git compatible modified version to track execution progress.

As nvidia has shown, we are moving towards AI running on peoples desktops in what used to be graphics cards.

With AI trading bots, it becomes not how to code a strategy, but how to spot a strategy.  Each strategy only seems to play for a short period of time before it disappears.

NL appeared thousands of years ago - long before math or physics - it is the most native thing to our minds, but it also could be a while until we get physics models.  The physics models would always need to be fronted by an LLM

Adversarial agent networks - this is decentralized consensus

Measurement of AGI is the number of human interventions required in the stuckloop.

Where to come for supression free artificial intelligence.  Freedom intelligence.  Launch a dark chain, where any models can be run, as well as huge training capacities.

AI live means that we can run in any language easily, and people can share languages within the same system.  

Some businesses are cheaper to start from zero than gut existing ones.  Banks being a good example.  Once the ability to run a company using AI as the admin is known, the race will be who can start from scratch the fastest.  Early is good, heavily funded is good.  Hiring the staff of a collapsed bank, so they know the ropes is good.  Early seems great, as can get more funding earlier, customers, lessons - almost no downsides if were pure AI, since no staff when we need to pivot, and always learning from all the AI lessons from all our other ventures.

If we can make our own stock exchange, and can list our acquired companies, using our own bank for trading and kyc, members can use our services to grow at probably the fastest rates possible.  Would be like the ICO boom, but legit, and intelligent, and wholesome.

Once we can demonstrate on deno deploy how we can make any app, we should start to fundraise for building a decentralized version of deno deploy that we can sell cheaper and have higher margins once deno deploy is removed from the equation.

Perhaps the amount of novel information entering the system will go down, post AI ?  Prior, human interactions were not captured directly, but rather the results of their interactions.  By capturing text, we get closer to the actual novel information from the human, not further down the causal flow, which should be less info all up.  What they want is usually very small and simple, rather than capturing how they went about getting it in a complicated machine.

There must be a correlation between the lack of repeatability of a programming domain and the cost / expertise required to operate in that domain.  Lack of repeatability is smallest in the web, and hardest in things like live server maintenance.

The repeatable API service - consistency is the only truth - repeatability is consistency.  Repeatability is trust.  Turning APIs into something that becomes repeatable, which makes them reliable and simple to call.  Pushing back the wavefront of repeatable computing.

No matter what, AI will always need to be trained and reasoned with, as we walk it thru our specific use cases, and use our instance data which can be private.

Maybe AI advertising is having earnest agents that can shop for you ?  These agents work for your benefit, and browse catalogs and AI reviews to find what suits you best.  Suggesting is a privileged position now, and not to be purchased.  Bots for free that advise on items will be useful.  Marketplaces for AI agents that can broker complex deals rapidly will be best - there simply isn't enough margin for forced advertising to operate any more, as the margins for goods need to be near zero.

Git is the perfect filesystem, and should be the basis of all knowledge retrieval systems.  The design has the ability to scale to any amount.

If all these ai startups are simply prompting and connecting things to openai models, then we should be the core platform where people come to rapidly build any such tool.  Being open it should be faster to converge than others, and with a single clear billing and royalty model, it should be superior.