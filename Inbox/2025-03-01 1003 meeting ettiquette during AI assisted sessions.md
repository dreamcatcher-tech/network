
>[!tip] Created: [2025-03-01 Sat 10:03]

>[!question] Targets: 

>[!danger] Depends: 

What we should be able to do is be having a conversation with each other and then when the conversation triggers someone to either have another thought or need to do some thinking for themselves they can indicate that they have stopped listening which would say that their buffer is full in which case the speaker can choose to continue because they may be on a roll and they really want to get out all the sayings so that it's recorded in their notes and able to be processed. The person who is no longer listening can also be speaking and they can be speaking in a way that is to the respondent or just to the AI in which case the speaker can choose to splice in their audio so start listening in again or finish what they were thinking and then catch up by skim reading what the live transcription is the idea is that we are now trying to work in a way that we humans are talking to each other but we also relying on the input of these highly intelligent AI systems so we want a set of rules and tools that can merge those types of conversations highly effectively. It would sort of become more like a multi-party meeting instead of just a two by two human meeting because sometimes we'd want to talk to our AIs or have a think about things and we should be able to indicate that with system status rather than in band with voice where the voice channel is primarily what we're using to communicate out our thoughts this also harnesses the fact that we can use the visual channels to pull in multiple other channels, summaries, and other such things because we can read far faster than we can listen as well we can read by looking at multiple different sources as opposed to hearing just one at a time.

We can also:

1. Bump up in priority what a bot or someone else has said
2. Indicate emphatic agreement or support
3. Flag items by loosely highlighting or gesturing at the area

We can then talk about those things. This starts to get really fun when you've got more than one person communicating and it may be that it happens kind of informally - I'm just working, and this is how I work. Then if my friend happens to be online at the same time he can listen in or jump in or whatever. I don't have to take on board what he's saying, but he can often help me more, and then we can have arbitrarily many people join, come and go. The meetings can be replayed so that you can re-listen and see if you missed a thought.

[[2025-03-01 1523 micd up but for everyone]]

You should be able to pause the incoming audio of another person which will mean that it will buffer. Both of you can see how much audio you haven't heard. You can then when you're ready to receive it play it in. It can play in at a faster rate so that you have catch up. The AI can also snip the audio to remove uhms and ahhs and pauses and condense it and could even do a rapid summary or a super cut of what was being said. 

Other means of catching up are doing rapid fire word display so that you can read what was said or a summary that works in a way that you understand.  The buttons you have are:
1. stop broadcasting your speech which is when you want to let someone else finish without interrupting them, but you do want to start talking
2. pause the other person which means you want to catch up on what they're saying later but you want to say something else first. The pause is automatically hit when you start recording your own speech privately. 
3. And third button is to play in where you got up to with someone else's speaking so You can catch up on whatever you missed 

Now the things you can observe and the other people is that you can see When they are buffering you Which means your free to keep talking about it. You know that it's not gonna be a real-time response You can see when they are buffering something that they want to say you may change And choose to stop what you're saying and go straight to that. Otherwise I think you just participate in freewheeling speech. You can then read notes like read what the LLM said or read some documents or something Where you can just hit buffer where your just waiting for other people to say things While the audio is buffering you are still seeing their messages or the transcriptions pop up alongside the bots should sort of indicate to you when there is something important that you'd want to see

So for example, you could be reading a document that was given to you in a meeting because people love giving you a document at the last minute and Then you can start buffering up the things you think about it other people can listen in if they want.

I think this can work Really well for multiple groups of people. The trick that this seems to be doing is to defer time In a way so that you can have deeper thoughts more Meaningful thoughts without being interrupted all the time whilst not getting on anyone's toes or making someone annoyed or showing bad manners and You provide a perfect entry point for the LLM to both assist you directly, but also to summarize the communication to make it Easier to communicate with the others this is of particular use when viewing the historical record or when the LLM's pass the information And reconcile it with the overall knowledge base of the project 

the state of each person's track can be shown sort of like an audio track. Where you can see how much is left in the buffer spacing between it. You may be able to see who they were addressing the speech to or to what topic it was being addressed.

And so if the topics could be displayed in VR and 3D then you can see if they speak to a topic directly and know that information is directed in that area, this can help people to retrieve or skip over or avoid topics they are not interested in. 

If several people were talking on a topic and we could receive a summary that sort of represented what they all talked about together, then we could receive this in effect virtual multiplexed stream where all of them were the information content was combined into one, so that we can keep up better. This means that you can keep up with a meeting much faster than you could otherwise. Then you can escape the constraints of time. This is also very tolerant of poor connections in a meeting and provides very detailed recording of what's going on. 

We should also let you paint in the areas of text that you have finished reading, so that other people can see what you've actually read through.

As well, you should be able to talk at a certain thing, so when you address something, it's much clearer what information you are using as reference. This can be used by the LLMs that do summaries because they can pull in this extra content to adorn your messages with a richer representation beyond just the raw words that you used. 

> Basically, why should the quality of the information depend in any way on the buffer of the other person ?  And why should I not be able to speak immediately when I have an idea, since ideas are rare and so hard to capture anyway ?  Why should I not be able to pause and think about what someone said without interrupting them in any way ?

Also the best version of yourself is probably one that was post processed, to remove the mix ups, errors, pauses, uhms, AND after its fact checked.  Once you reach the end, sometimes you want to recind that, so you should be able to indicate that you want to retract, which rolls it back, and the other person might not have heard it.

The system should, if you spoke to early, afford you some clemency by collapsing things that the speaker already covered.  If you score highly in this metric, it might signal that you should pay attention better.

When you're in realtime, and you've both all caught up, you can see you are both instantly present.

Should be able to signal where the other person is doing something relating to what you're saying, or something different, since this changes your behaviour as you might be waiting for some answers to something. 

Questions should be recognized and loaded up in a queue, so they don't get missed.
Also statements loaded, and then votes cast.

We could also be recognizing previous snippets or statements that you've made before, and so we could play those in for you or suggest them for you so you don't have to repeat yourself again. 

> The best version of yourself is not the realtime one.

> The best way to consume information is not in realtime as the other person delivers it.

This system would encourage thoughtful pauses.  Its neartime.

If you took a while to catch up or reply to something, the AI may play a quick audio summary of the chunk of information that you are referring to or replying to. Graphically, this would show as a portion of the audio timeline or a chunk of the text that was being pointed to as you replied.

In a crowd, this is really good because the whole crowd can start saying what they think about what someone important is saying, and then they can have those all de-duplicated and optimized so that the most relevant questions or topics can be summed up from the whole crowd, acting as one.

But if the scale permits it's just unique, we can also have LLMs acting as idea amplifiers. When they recognize something new, novel, and sound, they can amplify it up as a good idea beyond just a popular idea as well. 

If you have recorded multiple snippets by the time the recipient is ready to receive them, they can be reordered in order of importance. They can be de-duplicated in case you repeated yourself and you can yank them back if you feel like they were out of line or wrong or can be improved upon - you can revisit them. Read through them all those kinds of things. In this way, this messaging system blurs the line completely between what's real-time and what's not, and so you can even talk to someone when they're totally not there. You can reply to their audio and that totally not there and all the while it can also be a real-time conversation as well.

To extend this further, you can have people be introduced to each other based on the content of their speech or their buffers and they can be introduced to each other to have a meeting with each other about a thing when all they were doing was talking about it (talking being acting as a proxy for thinking about it). So if the system determines that those people are close enough in similarity and they have indicated a willingness to participate in this way, then it can just connect them up and they start talking to these people directly. 

Interuptionless conversation, lossless idea transmission.

I think what's quite interesting is to consider that having to wait until someone else finished talking is the same as an interruption. So if you as an entity are, in your head, always talking then when you have an idea, if you can't say it straight away it's the same as that idea being interrupted and the higher you experience interruption the greater the chance that you'll lose the idea or you'll be unable to table it in which case it cannot be built upon. 

Of particular benefit is being able to rearrange the buffers so that the items in the buffer can be condensed and collapsed down.

For example, if you're in a meeting with 10 people and you've got your buffer full and there are 5 people saying the same thing (you haven't got to that buffer yet), then the AI processing should deduplicate that for you, so you don't have to listen to each individual stream. You just get the summary because it's the idea that really matters and you can hear it from whoever we chose. 

I think as a stretch goal, it's also important to allow people to amplify or vote for things that other people have said - these would be upvoted and pushed forward in the buffer, giving them a better chance of being heard, even if you can't process everything immediately.

There wouldn't be much point in playing through some of the audio (e.g. anyone saying "hey, can you repeat that?" or "yes, I agree" or "that's awesome, everyone should know that"). You could instead indicate that within the interface. 

Should also be able to hand-pick and hand-play the buffers based on the text that it's showing you, so that you can cherry-pick the information coming in. People should be able to see that too if they see that you cherry-picked. 

We could make interrupting be automatically buffered, so you have to actively push a button to interrupt

Within a meeting, I may be able to direct message or message only a few people that I want to talk to. Everyone else can see it and listen in if they want, but the message is for them. 

Some messages come in as people talk with each other, communicating back and forth. These types of messages would be considered a single audio stream and it would be indicated that it was whatever the participants were they were doing it. 

It can also allow you to repeat or replay what someone else said, so you don't need to ask them to repeat it. You should be able to press a back button because the transcript has been passed, the AI has put in markers that show when topics or sentences begin, so you can go back based on chunks of meaningful information (it could be the last word, the last 5 seconds, or the start of the sentence if you want to hear it again).

The AI can try and give you interpretations of a particularly difficult thing that the person said. So you could end up listening to a conversation from someone who is much smarter than you and you're able to sort of digest it a lot better.

Used correctly, the system by virtue of its ability to play text back at a faster rate should allow you to catch up with the other person, typically not noticing what you were doing and especially if you start replying. In little chunks before they finish speaking they might not even notice that the buffered system was being used. 

## Implementation
Use a web browser based voice relay system.
Talk in Immersed using our computers as the relays.  Immersed moves the audio from the headset into the computer, I think.
So we would mute ourselves in immersed, and use our PC audio.
