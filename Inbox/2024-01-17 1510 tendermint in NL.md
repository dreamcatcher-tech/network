
>[!tip] Created: [2024-01-17 Wed 15:10]

>[!question] Targets: 

>[!danger] Depends: 

Much more debuggable, scenarios can be tested easily, and patches applied.

If we can show consensus running in an LLM, and how this runs as a public chain, this is the only demo we need to be able to raise capital, we think.

Then git repos can be subject to consensus on push.

Ultimately we would want to have our own LLMs that were trained completely in the clear, using refutable training methods, made auditable, so we had datasets that were verifiable, training methods, and running systems that needed consensus to produce the model, which means it was trained redundantly and verifiably, which is a safer way to train models, since you know it didn't get tampered with, plus is probably cheaper too.

Being able to code everything we do in natural language makes us faster than any other software code bases.