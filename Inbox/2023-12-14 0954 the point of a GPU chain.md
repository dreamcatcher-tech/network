
>[!tip] Created: [2023-12-14 Thu 09:54]

>[!question] Targets: 

>[!danger] Depends: 

Being able to have a complete app as a thing - the project, the issues, the data generated, the compute - seems far superior.  Current projects deployed in clouds are very sticky and hard to move around the place.  If switching hardware was just part of normal usage, and it switches hardware technically multiple times a second, then you can never get stuck somewhere.

So seems building the GPU chain should be the top priority ?
Then everything the cloud providers provide of value, we can replicate it in a decentralized way.
So we would be a cloud provider that was entirely decentralized and running on blockchain with distributed hardware.  We would have some certified DCs but would also allow public hardware and highly untrusted hardware.  We would work to enable technologies like confidential computing to expand this model of trusted computation on untrusted hardware.

Ideally be a complete end to end solution for continually user improved no code AI dapps.

As the data from usage becomes part of the app
Used to be a clean separation between the app and the data generated with the app.  This is now the same thing and will be subject to harmonic motion the same as everything else in the world.

Allows for public fine tuning jobs to be done on the stuck loop, since a publisher of helps would be able to generate fine tunings for all the stuck:help pairs it curates, giving an instant response.

We can offer reliable unstoppable generation, without broken / failed API requests.  Particularly as the process gets snapshotted along the way, so the consensus swarm runs with resilience, and if doing more than a single run, if a member of the swarm drops off part way along, others can catch up quickly, so that the tip is always shared by multiple members.  Means that any one can drop and the process will continue reliably, even tho all members are unreliable.  The generation process can be purposefully throttled to allow reasonable catch up time without affecting output.  During live consensus we can checkpoint periodically where any catch ups can restart from part way along.  Leverages that a generation path typically takes a lot of GPU, and probably will always be comp heavy and slow.  Using this GPU energy, we can fortify the correctness of the network.

Phases:
1. HAL demo for the CRM showing the flexibility of the stuck loop and AI driving blockchain
2. Show a decentralized app being driven by a centralized AI, and generating stucks as NFTs
3. Show a decentralized AI driving a decentralized app
4. show an autonomous agent given funds to spend and no way to stop it

All along the way, show revenue back to the NFTs representing the tasks.
Produce a booklet that describes the process, and show the NFT representing the book itself.
Show the gantt chart of the NFTs we want to build out.


Means we need an NFT creator as the first thing we make.

Every piece of code running in its own isolation boundary.
Data sovereignty is always a wanted thing.
Endlessly growing memory that can optionally be shared is always desired.
Resilience of systems in the face of hardware and network failures is desired.
Vendor lockin is always abhorrent.

We should be able to, if we create the NFT system and the logic modelling system, be able to raise capital and convince talent to assist us.  Once we can do this, we can raise a pubco to boost our efforts.  Participants putting their mon

It is important to separate the effort from the pubco, since the pubco would be taking on liability if it was in control of the project - it is simply a powerful peer of the network.

Should be able to set up a particular model, and sell responses to requests.  People pay more for timeliness of responses.  We would handle prioritizing the queue, running requests in parallel.  Means that if you have set up a full blown instance of something, like Mixtral, then you would put our software in front of it to sell off your excess capacity, and also add resilience to your application since you can use the network if you go offline.  Leverages the deterministic settings on many models so you can be slashed if you provide erroneous answers.  Allows us to run models that are not separated out into layers to run on consumer GPUs, as this processing takes some work to do.

Is there another kind of AI, not LLM, and how will we respond to it ?

Its really about getting the NFT tasks system up and working, with Dreamcatcher LLM as the primary goal of all our efforts.  After that, we can kick back and simply architect, without having to do all the code ourselves - we become amplifiable.  Amplifiable by the LLM tooling we have captured, and by the social networks that begin to support us with innovation and cash.

Using layered models, we can pack out idle space in GPUs to run multiple models.

We can see multiple projects aiming to pack modern LLMs into crypto mining rigs, and this trend will deepen.  Also aiming to split apart LLMs to run on multiple machines effectively.  So this trend to break apart and to compact is good for us.  The way the LLMs run, on tensor and torch seems long term, as making new training frameworks seems exceptionally costly.  Further, we see an explosion in the number of models being trained, and the cost per model is going up, asking for a system where excess GPU capacity can be traded to train big models.  The rewards for this effort might benefit from being more than direct payment, and allow projects to attract investment in GPU effort as a currency, which can become tradable.  Tradable NFTs for GPU units contributed to a project, where those NFTs receive a royalty once the model becomes usable.  Dreamcatcher as the Meta LLM will allow it to be run and charged for once trained.


At the same time, concerns over fair recompense for training data is causing consumers to shield their data from training sets, and we think ambient attribution can open that up some.

> Generate NFT rewards for GPU contribution, which becomes tradeable and attracts royalty

Can reward some parts of the model higher than others, rather than just raw GPU ?

Making a large distributed game with AI involvement would be expensive, and so using player hardware, and rewarding them for mining, letting them pay each other for resources is better than trying to get them to pay a central resource.  So to make themselves function better, they might invest in their own hardware, or they would pay a bit more to get lively queries with redundancy.

People can set up smart contracts to pay cash for GPU hours to be scavenged, if they think a project is worthwhile.  They can buy the NFT once created to incent its production, or they can purchase its production at current market rate.

The NFTs couldn't possibly be a security, since they represent decentralized effort, for a decentralized reward.

With this protocol, we can take flight and float above the computing layer in a decentralized manner, which is not something the central LLMs can do, much less they can't take their databases and application delivery with them.  If we can deliver apps that are running purely decentralized, we have a capability they cannot reach.  Moreover if our features leant on this distributed nature to be more dynamic with wider reach, we then are providing superior features, plus decentralized trust.

All the solutions we are seeing right now are isolated point solutions - to make a global solution with wikipedia scale collaboration, we need a public chain to run all this on.

Want to supply advanced applications including AI with the same decentralized finality that Ethereum provides, but with limitless resources and near zero latency.  Currently the app infrastructure has to be hosted using web 2.0 and weakly hooked to the blockchain stuff - the whole app should be on chain.

If we make a small super fast LLM that is optimized for function calling, then we can ship it small enough to run on a users local machine in isolation.  We can start charging for its usage instantly, for every time the program runs, rather than currently where charging for running resources is considered trivial, since the user has their own computer that is mostly idle.

Being able to train a model using pooled resources, or idle capacity seems very useful.  The power to run any model in a distributed on demand elastic, and repeatable / auditable way is important.  Verified training is important too.  Huge scale is needed, but also the ability to quickly, reliably, and in a trustworthy way, pool resources.  Pooled compute is the general thing we are supplying, it just so happens we can use it for AI compute as well, so long as it is delivered in a repeatable way.  "Repeatable computer" is the key to trust, and if the AI can be run in such a way too, this is just a far greater form of utility of the same old thing.  Repeatable means we can run it concurrently, to give low latency, redundancy, and trust from untrusted parties.  Also enables edge, since we can run things close to the consumer then verify them later when resources are cheap.  Repeatable Computer can be easily moved around the place, since it technically exists in multiple locations already.