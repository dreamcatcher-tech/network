
>[!tip] Created: [2024-01-20 Sat 14:09]

>[!question] Targets: 

>[!danger] Depends: 

Because AI tests always take around 30 seconds or so to complete, we should make a workbench where changes cause tests to run where they can take a long time, but we don't care much since its always rerunning in the background and giving feedback on how far thru it is at each stage by way of a display widget.

Display widget would be stackable, and be bonded to a particular path in the fs, and know how to load up its children too.  Would be aware of the dirty and committed state of the data, as well as knowing when something hasn't been run yet.

These tests should be able to run on a remote server to get good reliable thruput where the client can drop off easily.

How do I add a customer to a CRM ?
How do I make a template ? change a template ? update all records ?

Make some example imports using XML that has been altered to remove all sensitive data.

Then get the stuckloop going so we can get goals logged on the git fs.
Sync this up to a repo so we can start to digest it using tooling.
Then when we insert fixes and they get used, they are immediately apparent.

Then move to using embeddings rather than megaprompt for selection of the help.

Overall system integrations can experiment with different kinds of goalie and different help formats.

How to do tendermint in NL ?