
>[!tip] Created: [2023-10-09 Mon 08:48]

>[!question] Targets: 

>[!danger] Depends: 

GPT4 appears to be able to give some meaningful results as to who is due what amount of each file.

We would then make a codepath sampler or tester that can determine what codepaths were run each time.  GPT4 would then tell us who was responsible for what value in the average execution of the code.  

The algo would be subject to prompt corrections where the humans would reason about different things but also the algo would be subject to corrections from humans too, which is a higher level.

So we could probably start running now, with some initial agents built using initial arguments from various parties, with GPT playing arbiter between them. The participants would be engaging in this model of attribution, and would be also committing to upgrades to the model over time, with recalculations being made frequently.

Interrogation UI where you can point at a specific section of code, and ask why it was rated the way it was.  Perhaps get a line rating for value to code, and then a line rating for the royalty split to all contributors.  Start by analyzing popular open source projects.  If a few of these projects could switch to us, then it could be used by them to disperse patreon funds.

So it doesn't have to be perfect to start, just needs to show that it does intelligently work, which proves that it will get better over time.  From launch this is already better than the current situation of no royalties, or options agreements that drag out the process and might be wrong.

Our goal is to remain the top selected attribution algo - as buyers choose other ones, a balance will be struck.

Ultimately we want a GPT agent that can marshall between upgrades to the core prompts based on logic.  Then these machines can be truly decentralized.  We can start with a version of this now, as the bad versions will just die as people will no longer support them with resources.

The construction of an AI capable of deciding when upgrades should be allowed can be fully autonomous.

Fallacy is thinking about an agent as one thing, when it can act as many adversarial systems.

People could pay for a gpt4 run to assess code outcome, which is submitted as part of a consensus run, since we need say 4 to accept the output.  Then the outcome becomes an embedding, and so debates can be held from that point on.  Each run needs to store its results in some gpt4 compatible way.