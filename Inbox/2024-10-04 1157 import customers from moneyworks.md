
>[!tip] Created: [2024-10-04 Fri 11:57]

>[!question] Targets: 

>[!danger] Depends: 

Should the AI be in charge of writing the fetch parameters, to be able to pull things in from moneyworks ?

Or should the AI be in charge of writing these parameters, storing them as json, getting approval from a human, and then using those parameters repeatedly ?

So the AI helps you get the parameters right, and then lets you store them somewhere, then it writes them into a json object that creates and isolate that is actually dynamic, stored in the thread state, and so it is live programmable.

Pulling in the xml file is just a fetch command.
We should make the local agent be simply a proxy service, rather than the full operating artifact ?

Or a local agent could run, fully secure, with no leaks whatsoever, browsers connect to the local URL.  Cloud is easiest to debug tho.

How can we check the import, if we processed the records using AI ?
Do the same transform twice, but in a different way, with different models ?
Take the markdown, generate compliant xml, and see if they are the exact same ?
The reversal principle, that demonstrates complete control of the process.

So an agent could be running that simply allows a web push and request to be made at the behest of the remote artifact instance.  This thing could also provide a tunnel for all web api requests to go thru, rather than requiring cloudflare ?

Using cloudflare would be better since the app could be used remotely, too.