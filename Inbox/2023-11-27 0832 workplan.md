
>[!tip] Created: [2023-11-27 Mon 08:32]

>[!question] Targets: 

>[!danger] Depends: 


~~react component remount errors~~
jest test for doing add customer
test suite in storybook
redo schemas to be a top level key for collections
assessor based testing
nice human readable book of knowledge in markdown format added to retrievals
router component to choose where in the filesystem to drop
one bot per filesystem path
tool to modify the assistants on openai platform
autogen of the book manual thing
be able to present multiple versions of the system for testing, with each one treated as a rig.
present a fork of artifact so we can rapidly iterate on a complex filesystem with new AI layers

Goal here is to enable Maxwell to work with the prompts to get the tests to pass, and the scores to succeed.

Ultimately want to be able to feed the tests back into the AI so it can generate.  ASAP we want reliable AI tooling to build the unreliable parts of the system, so we are getting at least some performance boost back.

command to delete an assistant / reload it on the openai platform.
goalie UI tester in storybook - get SM to start testing the goaling bot
display functions using the actions panels
reducers should not repeat, to make logging and debugging easier
set state of errors in terms of what actions led to the error - relates to [[2023-12-01 1258 tracing consequences of actions|consequence tracing]]
schema as first class member.
automated tests / appraisals.
UI for the automated tests of bulk changes
Present this as a jitter - the storybook should be able to preselect what the jitter is so you only need to input the data, or have it automatically begin processing so you just refresh the page and it starts operating.  The tests should be run inside a jitter, not their own item.

Draw on the stateboard as the normal way to do things.
Show the filesystem in the stateboard.




system prompt writing loop - provide a test for an assistant, and say what is lacking / how to assess success, then task the AI with iterating on how to make an assistant prompt that passes the tests we set.  Be able to watch progress and to interject with chat comments as to what you think should be different, then have it run a bunch of iterations as it tries to improve.

Detect when it isn't getting much better after N times, and show a graph of progress to express its issues, which it shows to the user asking for some help.  This will enable our prompt creation studio - a forge - which can help us get the quality of our bots up high.

The storybook for automated tests would auto load the md file, process it, show these results on screen, and begin running all the tests based on some system change.

Once we have permanent storage, you would edit the md file, and it would auto trigger the run, and you could come back and check on how it was doing at any point, or be notified when it finishes.