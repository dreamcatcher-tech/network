
>[!tip] Created: [2025-01-26 Sun 12:44]

>[!question] Targets: 

>[!danger] Depends: 

Getting the same response several times is the hallmark of accuracy - the repeatability component of the system.  A knowledge ssytem with good repeatability means and equivalent question will give the same result.

With reasoning, the range of answers should look like a comb, where there are islands of answers.

For any model, if we run it an unlimited number of times, we should see this same comb filter response.

We should then be able to extract out the essence of the information contained within.

Then if we refeed it back in, to use as seed for further generation, perhaps we can refine the accuracy again ?

We are talkinga bout making a set of inner bieves that the moel can use to reason about advanced topics.  These should be revisted periodically as new infor becomes apparent.

Further, we should detect when the same thing has been asked, and we should save the tokens and pull out the atomic flared reasoning that has occured.  If we then break the question up, using flaring to get the breaks right, and then reduce it down to things we've ground over before - common ground - this will give fast, accurate results.

This is a knowledge system bolted atop and intelligence.

Preferences for the user can be included there too, so the bot is now choosing from things people have liked in the past.

If you cause a grinding to occur, whilst you do pay for it, we can subsidize it, and you earn credit for it.

The bot can also ask you to provide verified info, that you will get paid for.

Becomes like a wikipedia for bots to use.  The canonical RAG system of knowledge, plus has stake to say, plus can say what is ground truth, and can store the ground truth outputs of all popular models.

The source of truth for AI should be singular, decentralized, and vast.

This can then become the training data for the next gen.

Conflicts are highlighted, and humans help settle these differences.