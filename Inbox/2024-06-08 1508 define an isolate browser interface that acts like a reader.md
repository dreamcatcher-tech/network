
>[!tip] Created: [2024-06-08 Sat 15:08]

>[!question] Targets: 

>[!danger] Depends: 

Means that we can set up api services that can be called upon in the bots.

They are managed in an NL catalog, and example calls are able to be seen.

Do trusted url retrievals, where using an isolated runner, a url request is performed, and the results committed to public ledger.  Means that it got retrieved from something beyond reproach.
Can also have independent runners submit their results too.
Plus submit to webarchive at the same time.

Using browserbase.com we could integrate a live view of the page as a jitter and so they can see what we're doing, and potentially interact with the session directly.

Might be possible to replay sessions that have already run so the user can see what happened.
Being able to debug the session could be useful.
Save the debug or the raw dom in a way that cannot be refuted.

Might be able to use this integration with our browser testing framework, to load up our own webpages, in different browsers, and see if they operate correctly ?
If we use browserstack to ensure the basic connectivity works, then we can use the chrome browser to test the intelligence is working correctly.
Then replay the session.

There is a big market here for deduplicating scrapes.  Turn a scrape into a solemn and reference certified act running in a github isolate.  This means that everyone else can consume this.  So pay people to produce these scrapes and stake against it.  Then everyone just reads the hash, rather than running abrowser directly.  Means can run offlive as can pull down all scrapes of the sites you might want, then process them at your liesure.  Also can use as proofs.