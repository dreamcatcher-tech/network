
>[!tip] Created: [2023-09-06 Wed 11:28]

>[!question] Targets: 

>[!danger] Depends: 

Each time a user submits a behaviour that they think should be happening, but isn't, we should be able to distill this down to an example machine, and that machine should be used as a test case to verify each release.

Also searching by state machine nearness.  If you make a simple state machine, then it will bring up  machines and bug reports that are similar, and ones that have sections that are similar, then highlight the area that caused it to show up in your search.

All state machines should be part of a global space, so we should amalgamate everything from the tests cases and other things, which makes a searchable database.

Then we can show what functions of the core get used the most, since those are state machines too, so we can see which runs run white hot, vs those that hardly get used at all.

Then if an AI could generate state machines from plain text, you could highlight any section of the documentation and ask for a state machine to be constructed that demonstrated what the text was about.