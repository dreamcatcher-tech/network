
>[!tip] Created: [2024-12-18 Wed 15:04]

>[!question] Targets: 

>[!danger] Depends: 

Have an eval of the pass, so that on completion of a loop, it checks how well it did, and then refines it after running the code.

Should be able to run a linter on the code, and give back those errors to the model.

Some kind of cross project files, like exported interfaces, or some other important guidance.

Need to give it the concept of files, and a branch.

We should totally separate out the operations branches from the code branches, and at best mention them in the commit that occurs, or in some file that gets stored, like in a `.artifact` file.

Really want to break something down into a study, and do a tool call, where the study has an outcome that gets appriased for suitability.

Inside a study, a new study can be generated, but the format is still the same - starting parallel isolated lines of development to solve a particular thing.

Constantly update the issues / requirements list as the human understands more about what they're asking based on the renders.

This grinding method is the same for legal analysis as it is for code.  These loops simply repeat, with an appraisal of when a new study might need to be kicked off.

These can then be viewed in some kind of web display, so you can see what's happening.
These don't need streaming, and if they did, we'd set up our own server cluster to relay this, since it is a nicety and not an essential thing.  Really just needs the browser to set up a webrtc connection that the server side can push to no matter where it is executing from.

Basically the key is recognizing when a study needs to be started, and being able to fork out multiple studies.  then loop each one, and decide if this path is viable, if other paths can now be explored, and if we should do another refinement loop.

Present a best solution at any given moment.

Be able to set a price target for the study.

Summarize multiple findings, and get the human to unstuck the machine.