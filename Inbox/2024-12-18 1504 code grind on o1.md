
>[!tip] Created: [2024-12-18 Wed 15:04]

>[!question] Targets: 

>[!danger] Depends: 

Have an eval of the pass, so that on completion of a loop, it checks how well it did, and then refines it after running the code.

Should be able to run a linter on the code, and give back those errors to the model.

Some kind of cross project files, like exported interfaces, or some other important guidance.

Need to give it the concept of files, and a branch.

We should totally separate out the operations branches from the code branches, and at best mention them in the commit that occurs, or in some file that gets stored, like in a `.artifact` file.

Really want to break something down into a study, and do a tool call, where the study has an outcome that gets appriased for suitability.

Inside a study, a new study can be generated, but the format is still the same - starting parallel isolated lines of development to solve a particular thing.

Constantly update the issues / requirements list as the human understands more about what they're asking based on the renders.