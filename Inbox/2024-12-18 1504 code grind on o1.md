
>[!tip] Created: [2024-12-18 Wed 15:04]

>[!question] Targets: 

>[!danger] Depends: 

Have an eval of the pass, so that on completion of a loop, it checks how well it did, and then refines it after running the code.

Should be able to run a linter on the code, and give back those errors to the model.

Some kind of cross project files, like exported interfaces, or some other important guidance.

Need to give it the concept of files, and a branch.

We should totally separate out the operations branches from the code branches, and at best mention them in the commit that occurs, or in some file that gets stored, like in a `.artifact` file.

Really want to break something down into a study, and do a tool call, where the study has an outcome that gets appriased for suitability.

Inside a study, a new study can be generated, but the format is still the same - starting parallel isolated lines of development to solve a particular thing.

Constantly update the issues / requirements list as the human understands more about what they're asking based on the renders.

This grinding method is the same for legal analysis as it is for code.  These loops simply repeat, with an appraisal of when a new study might need to be kicked off.

These can then be viewed in some kind of web display, so you can see what's happening.
These don't need streaming, and if they did, we'd set up our own server cluster to relay this, since it is a nicety and not an essential thing.  Really just needs the browser to set up a webrtc connection that the server side can push to no matter where it is executing from.

Basically the key is recognizing when a study needs to be started, and being able to fork out multiple studies.  then loop each one, and decide if this path is viable, if other paths can now be explored, and if we should do another refinement loop.

Present a best solution at any given moment.

Be able to set a price target for the study.

Summarize multiple findings, and get the human to unstuck the machine.

Set some basic tests, and tell the bot to grind until it has a good outcome.  Can describe what the observed result should be, so it will know without having the exact output, which the dev wouldn't know.

Get it to stop when it needs more info, like asking for specific laws of nz.
Be able to pass it a db call, where it can grab these kinds of things, like a filesystem that it could walk and get summaries about, and using some kind of categorization.
So it should be able to dispatch a seeking agent that will go and find the laws it needs.
Would update the search context, so when one agent finds an area that the task seems to need, it can tell the others it found it, rather than getting each one to do their own search.

There would be a clone of the repo sitting in artifact.  Both of us would have access to it.  Then when we start a new thread, and we want to make some filesystem changes, we make a new branch with some auto generated name for the intention of the fork.  Then when we're happy, we would PR against the base, either in github, or on artifact.  Use o1 solves to resolve merge conflicts with main.  Generate very helpful commit messages.

use o1 to figure out what the relevant parts of a codebase are, so that it isn't stuffing everything in the context window, and so it can remain focused.

Generate a summary of the whole codebase so even a small query can come with the summarized form of the codebase.  Might use a dependency graph, but this is not consumable by humans easily, unless graphically, so graphical might cause o1 to unreason, or be hard to relate.  Mermaid might be a good compromise.

The good thing about git is that it lets us sandbox the computer, and discard if things go wrong, plus analyze the failures.