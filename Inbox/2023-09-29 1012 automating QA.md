
>[!tip] Created: [2023-09-29 Fri 10:12]

>[!question] Targets: 

>[!danger] Depends: 

If the QA is an AI to start with, and it gives hints and issues with the document before it is submitted.  Down the side is a list of identified issues in a standard form.  This issues list can be expanded.  It also links to parts of the document identified as problematic.

So a user can see what the QA will see, and can see suggestions about how to fix it, as well as some examples that do the thing QA is looking for well.

The QA should be controlled, such as having a list of principles, and then each time the QA acts, the LLM is asked for a boolean - did the LLM adhere to this list of principles or not.  Here the LLM is acting as a trust arbiter, so that participants know that whilst the exact output requires the intellect of a human, the human is guaranteed to behave in a way governed by the principles advertised ahead of time.

This highly constrains the QA, prevents them going rogue, and gives participants a great safety relief, since they need to know and trust the QA even less.

QA for edit can automatically score changes as changing the meaningfulness more than, say, 10%.
Reason is given as to why the change is incremental and not material enough.

The dispute information is highly useful since there were gas costs and QA costs involved, so it must have been valued by at least someone.