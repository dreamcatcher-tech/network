
>[!tip] Created: [2025-09-01 Mon 10:35]

>[!question] Targets: 

>[!danger] Depends: 

https://enegames.itch.io/thunder-lizard-eat-or-be-eaten
https://blog.jeffschomay.com/rendering-a-game-in-real-time-with-ai
basically the limits of ai upscaling, but in the opposite directly.

the raft of utility in cli / terminal apps that we can instantly browserify and vibe interface with.

strudel: https://www.youtube.com/watch?v=ZCcpWzhekEY

telnet doom.w-graj.net 666
ssh -C 666@sshdoom.megidish.net using sixel

all watchers of a view getting navigated away too.
or what about all turning their mics on ?

design decisions:
1. agent workspace config, like sysprompt and others can only be set at creation time
2. a face is used to get a job done, which may be multi turn

What's hard is to think that all the concepts we use have to also be presented to the model too - conventionally we the programmers held the concepts in our minds, but now, we have to share them with the llms

the agent interactions seem exclusively strings, rather than schema'd function calls.

even with a perfect model, it probably comes down to about 40% harness, as to the quality of the performance.  Plus we can see that with how the Agent from openai, and deep research, score much higher in hard benchmarks than the raw model.