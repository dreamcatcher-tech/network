
>[!tip] Created: [2023-09-30 Sat 23:10]

>[!question] Targets: 

>[!danger] Depends: 

The data stream of the prompt should be saved as this is valuable to have.

If the UI could interpret prompts from gpt, such as graphcs of tokens held, then we could allow for more general questions about the state of things, such as "what do you think the value of this thing will be in 2 years ?" and "what is the sum of the top 5 packets ?"

Can use typescript to force a format to be met, and allow it to keep trying until it gets it right.

Let it plot charts about how groups of packets have progressed.

Can develop your own dashboard homepage with charts that you have defined in natural language, and control the layout using natural language.

So even adjusting a window should be like "little bit left" and have the object representing the window layout be adjusted slightly by the ai prompter, which is then interpreted by the UI.

So by default, the whole app is laid out in natural language, with the UI elements just rendering what the language machine sent back.  Means we can get people to chat up some UI elements of their own, making them creative.

Particularly since the graphcs can be generated by image generators, we can make highly customized UIs.  Then can sent it off to some human for some polish.

So the chat bot would be the help, as well as the controller of it.

There could be a component definition language that is almost text based that can be used to generate UIs.  Almost be like a form of mermaid diagram by for NLP interfaces, so buttons, forms, scroll boxes.

"Always show me graphs as pie charts" should know to update a global register of preferences, which are stored as natural language, and used to decide how to output the current UI state.
"make the screen show dark".

Preferences being always natural language is better, since it is more user intent based.
We could interpret requests and normalize them against common settings others use.

Screen is like a terminal screen, where each screen is fully redrawn on every change.

Could probably hold the whole UI state within the AI model in ram, and so changes to it via NLP can be with clicking on a gui that triggers an NLP instruction, or using NLP.
This means that user stories can be specied as natural language, and perhaps an application can be generated from just this description.

Should be able to auto generate a state chart for any GUI that was described in this markdown / yml style format.

"take a whitelist of what tokens to show from site xyz"