
>[!tip] Created: [2023-09-30 Sat 23:10]

>[!question] Targets: 

>[!danger] Depends: 

The data stream of the prompt should be saved as this is valuable to have.

If the UI could interpret prompts from gpt, such as graphcs of tokens held, then we could allow for more general questions about the state of things, such as "what do you think the value of this thing will be in 2 years ?" and "what is the sum of the top 5 packets ?"

Can use typescript to force a format to be met, and allow it to keep trying until it gets it right.

Let it plot charts about how groups of packets have progressed.

Can develop your own dashboard homepage with charts that you have defined in natural language, and control the layout using natural language.

So even adjusting a window should be like "little bit left" and have the object representing the window layout be adjusted slightly by the ai prompter, which is then interpreted by the UI.

So by default, the whole app is laid out in natural language, with the UI elements just rendering what the language machine sent back.  Means we can get people to chat up some UI elements of their own, making them creative.

Particularly since the graphcs can be generated by image generators, we can make highly customized UIs.  Then can sent it off to some human for some polish.

So the chat bot would be the help, as well as the controller of it.

There could be a component definition language that is almost text based that can be used to generate UIs.  Almost be like a form of mermaid diagram by for NLP interfaces, so buttons, forms, scroll boxes.

"Always show me graphs as pie charts" should know to update a global register of preferences, which are stored as natural language, and used to decide how to output the current UI state.
"make the screen show dark".

Preferences being always natural language is better, since it is more user intent based.
We could interpret requests and normalize them against common settings others use.

Screen is like a terminal screen, where each screen is fully redrawn on every change.

Could probably hold the whole UI state within the AI model in ram, and so changes to it via NLP can be with clicking on a gui that triggers an NLP instruction, or using NLP.
This means that user stories can be specied as natural language, and perhaps an application can be generated from just this description.

Should be able to auto generate a state chart for any GUI that was described in this markdown / yml style format.

"take a whitelist of what tokens to show from site xyz"

Blockchain aspect is important so we can do all the AI tricks offline, but also so we can share resources to allow anyone to contribute them, and so we can provide cheaper and faster response times, plus trust.  Also gives us a faster to build system, with AI built in by default, and global scalability built in.  But primarily it provides data self sovereignty to the users, and provenance of all changes.  Blockchain gives the rollback, simulate forwards aspect that is critical for an AI powered system - always be able to back out of your changes, no matter how large, and always be able to simulate what they changes will be, then apply them directly - this part is key for AI to be able to learn what it should have done, but also for humans to consider the complex changes an AI proposes to make.  Atomically being able to merge these in is critical.

Goal is that if this is the first framework you learned how to use, then you would think it insane to do any other way.  So ignoring the cost of learning, this would be a quicker framework, plus has a labour market built in.

Can switch out the UI that you are using.  Each selected LLM would need to pass a basic fitness test before being allowed to drive.

Changing preferences can be done like writing a packet - the target is the preferences file, which has a strict format, where new formats can be added in as people invent new things and names for things they want.  Then a translation to a standardized form for all.  When you ask for what you want, it will put up a diff of the preferences that it is about to change for your approval.

Key is that everything must be undoable, and also outcomes simulatable before commiting to anything.  Give people a strong sense of being in control, and that the AI won't do anything they were not ok with.

Traditional UI could be navigated with a prompt and response window down the bottom, that can move around the UI, but the whole UI is the target of the chat, and is made compatible with the prompt interactions.  eg: all text box edits are diff capable, all actions from the ai are undoable.  Navigation can be performed easily.  Some actions, like nav, the ai just takes since they are not change inducing to the permanent state.  Back button in browser undoes that.

An API should be an LLM into the system ?  Remote system describes what it wants, target simulates the output, and the remote system iterates until ready to commit.

Prompt gives you search ability too, like all the packets that mention "rain".

AI can generate little loop tools - when you begin a loop, it starts to detect what you are doing to each and every item in the list.  First you scope the list, then you start to operate on each one.  You might refine the scope more as you go along.  For the next items in the list, the AI is always guessing what it thinks you are doing, and so eventually you get to the stage of rapidly just okaying things.  This is in teach mode.

It should be able to detect a bunch of preferences, and distill them in a single place, like a set of rules.  "from now on, always address me as sir" - this should make an entry in the preferences file and be persistent across all prompts.  We can start with  just superloading the prompt, and move to embeddings.

Start with the chat bot interface presented on the website.  As you chat, it can navigate you to the most relevant section of the site.  It can clarify that some answers are about plans, since it knows how to read the roadmap nfts.  This window can be full screened, but it always looks like a modal, sitting on top of everything else.  Can be totally hidden if you want.  So we start with an educational piece, in the main website, but can rapidly jump off 

The UI is used to show information in a dense structured way, such as a list or some graphs of funding.  

Any time you ask the AI to do something, if it cannot do that it should immediately walk you into constructing a packet mode.

Let people mess with the programming of the prompts too, by using the prompts, to experiment rapidly.

Offer payments and explain how attribution works, so that if you pay for these API credits now, then people who use the free credits share with you some of their attribution.

If it doesn't know the answer to something, it should include in the prompt that it didn't know, which is a hidden flag that we can scan for when viewing everyones prompt interactions.

Components that present multiple areas, like a multipart form, should allow themselves to be focused, where we restrict the AI to processing just that section.
When zoomed back out, if the AI thinks it wants to change other sections, ask for permission.

Be able to list what packet formats it is applying, why did it apply this one, and what are some examples of this packet form.  Then narrow down those examples based on some search criteria.

So we're first and foremost building an AI tool that helps create packets, helps QA process the packets and provide helpful annotated feedback, and automates the discernment of a solution meeting a request (a solution solving a problem).  We show the feedback the AI would give to the QA to allow people to polish their submissions before sending to the QA at all.  This reduces the inefficiency of "bounded agents" as the processing time / cost, intimacy of feedback, and intelligence / standardization of feedback is boosted by an AI.  Also language barriers are overcome.  Using AI to overcome market inefficiencies in the knowledge work market.  With NFTs and blockchain.

Inefficiencies due to duplication of effort are rapidly removed using the AI that we launch with, in its most basic form.  It is a deep focused type of search that pushes towards making a new floating task that anyone can do.

Annotations on a body of text are given by the AI, then we pull those out, link to the text, and then show some hover over buttons on the main body, like google doc comments.

"show me the status of all my packets - any activity on any of them ?"