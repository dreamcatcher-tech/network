
>[!tip] Created: [2024-08-10 Sat 10:32]

>[!question] Targets: 

>[!danger] Depends: 

If we set up a schema that gives ways to think about a problem, some target goals, and then we can force a style of thinking on the bot.

The response would be the final part of the prompt, but the reasoning steps make it far more accurate.

User could see the reasoning steps first, whilst the longer prompt is being generated.
Or at least be able to show it later if they wanted to drill down.

Get the bot to select what behavioural rules seem most relevant to adhere to.

Can additionally stop the prompt part way thru and do some reassessment to keep incremental steering going.

Also allows the bot to answer the question "why did you say that ?" with more clarity, since its thinking is recorded.

The trick is to influence the final outcome with some thinking beforehand.

So if we combine this with HAL3 having 3 versions of the prompt output, made with varying temperature settings, then we can do parallel step by step thinking, select the best one.

Showing this in the UI would be fun, as the 3 results would come in at different times, then the selection would be made, then the rest of the prompt would be generated, possibly again in parallel.

We will probably always get better results from doing thinking in this way.