
>[!tip] Created: [2025-03-05 Wed 22:53]

>[!question] Targets: 

>[!danger] Depends: 

If we had in our archives every query that had ever been put into the Dreamcatcher, we should be able to:

1. Whenever anyone asks us to do something, pull up these examples or previous versions
2. Score them based on how well they did
3. Pick a good strategy or a good version, particularly if someone took the time to tweak a particular output and it has been used popularly with that kind of thing

Furthermore, every time that we give some kind of result we really ought to be running parallel approaches. The top ten strategies that sound like is what we should be running and then compare the results to see if any of them worked for us in this way. Here we can avoid the reliance on a core LLM to provide us high-quality answers and start drawing on the fact that the humans are giving us real-time feedback and sometimes the humans will happily configure or tweak the machine, train it, teach it, school it in ways that make the responses more favorable to the humans give it strategies link it to particular knowledge that has of strong importance rather than relying on the LLM to have learned it. We can also include some software tools computation and other such things that can even further enhance their quality of the response. These strategies seem almost limitless in number and will rapidly fill out the basics just like the MPM package library had its bases rapidly filled but they will always be growth and there will be an unlimited number of packages needed over time. Some will deprecate, some will grow, others will be spotted that can merge the same mechanics as what happens in the MPM ecosystem will seem to be applied in this strategy library.