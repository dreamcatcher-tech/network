### Taxation 
Taxation using object trade and ambient attribution - pay a certain fee to the govt, and pay a projection of your gains to govt.

Govt should be a pot that you have a vote on each time you pay into the pot, with the pot that you voted in.

### Business model for SaaS
To quote from [Nomadlist](https://nomadlist.com/faq#free): 

> I love free software and could not have built my site without it. But free web services are not like free software. If your free software project suddenly gets popular, you gain resources: testers, developers and people willing to pitch in. If your free website takes off, you lose resources. Your time is spent firefighting and your money all goes to the nice people at Linode.

We need a revenue model because web apps cost resources to run.  We offer a platform where the deployment of SaaS can mimick free software better, and it costs money based on usage.  The revenue model baseline is set immediately, so if the site explodes in popularity, you will make some money by default rather than lose it to AWS.

To further quote them:

> **Like a service? Make them charge you**Â or show you ads. If they won't do it, clone them and do it yourself. Soon you'll be the only game in town!

If we start to clone popular free sites, particularly those that are open source, and pay back to those sites, then anyone having problems with the cost of providing SaaS for free should soon switch over to us.  There is not a lot of free saas or microsaas sites out there - the incentives aren't there and micropayments aren't feasible, plus user pays software architecture isn't present.

Would be like AWS but it charges the user for the resources they consumed when they access them.

In our environment, usage graphs and metrics would come built in to the platform.  We embed privacy for the users too.  Charging for usage is done thru a gateway service, so this is unbundled from the site itself - if you just run the `npx` installer, you will automatically have a payment gateway for your users built in.

If not doing it for yourself, do it for the people who wrote all the code you depend on.  For the users, do it to incentivize continued improvment of the site.  We should be the preferred way to run a site as we can charge from day dot, or allow people to donate their resources in the browser to run the site.

Price discovery by offering a bidding system so can learn what people actually want to pay, and can forecast based on the interest shown.  All analytics is public, so anyone can learn from it.

## Offline first vs caching
Offline is similar to how companies behave anyway - in privacy they work, then the intermittently come together to trade resources.  Offline first is different in that at any moment in time, the app is working offline then seeks to sync with others.  At no point is it ever waiting for instructions from some remote party.  This is different to being offline tolerant, but still needing to reach quorum somewhere.

## Offline to combat climate change
Datacentres as highly vulnerable to climate events - overheating, power outages, flooding - all these things can take a DC offline.
The most climate friendly server is no server.  Reducing the server count increases environmental benefits, no e-waste, no power consumption, no incentive to build more DCs.  All while providing superior user experience.
Climate change affected regions lose connectivity, but have no excuse for losing functionality.
[Upwards trend in unreliability, and also costs is well documented](https://www.datacenterfrontier.com/cloud/article/11427329/uptime-longer-data-center-outages-are-becoming-more-common)
Goal is to takeoff in a plane, and while in the air an app specific to what you need has already been built, tested, and deployed.
[Heat waves affecting cooling](https://gizmodo.com/heat-waves-climate-change-data-center-server-shut-down-1849916741)

The amount of electricity used for computing is about 10% of global.  Much of this goes to the network, so reducing the network traffic is combatting climate change.
Carbon output for mfg of computers - show what this is for servers.  If we can remove servers, and network traffic, this is the biggest carbon emission reduction.

Using second hand computers in the enterprise.

Dollars as a carbon causer - the more IT costs, the more carbon is needed

Find estimates of duplicate data transmission.

Better quality software, cheaper, and faster.  Carbon emissions are being wasted during the time your software isn't deployed and solving the problem it needs to.  Costing more is wasting carbon too.
"Carbon is money" - if you're emitting CO2, then you're wasting money now, and will be penalized even further later.

## Complexity as the uptime enemy
The more complexity is added to a DC, as well as the redundancy systems, the more likely it is to fail catastrophically.  Also the recovery time is larger.  Simpler systems crash more predictably.
The most invulnerable datacentre is no datacentre.

Show studies that map DC outages, and show less frequent, but goes down for longer, with more puzzling outcomes.  The devil is in complexity.


## Cost as a proxy for Carbon output
If you are costing businesses $x, then there can be loose mapping made between GDP, CO2 emissions, and how much emissions you are causing.

## No formality needed
Examples of running large cost code, even human life, on non formal languages.  Arguably more critical value passes thru non formal code.
Just because blockchain is unalterable and public doesn't mean it needs to be formally verified.   Just because we cannot trace culprits does not necessitate formal languages.  Who is the culprit in a plane crash ? Despite this most code still runs using non formal languages.

Using the JSON format of the engine, we can run formal languages in lightweight containers and thereby provide the formal guarantees for important contracts too.
Can specify that we need a set diversity before finality, so some nodes run different versions of the software to ensure the swiss cheese model of security has many slices, covering all holes.

## Forking of code accutely in DeFi
The forking problem is more obvioius in the summer of DeFi

## Ancient need for attribution
There is an ancient animalistic force within all of us, that demands fair reward for our effort.  The management of this desire thruout the centuries has been problematic.  In capitalism this link is supposedly tied to our income, but we sense there is growing disbelief that this connection is strong enough any more - it appears to have drifted and reward appears to have been corrupted in its flow from the source of wealth to the consumers.

## Self interest in a closed box
Self interest economics in an unbounded system clearly work well based on the past half millenia, however when what we thought was unbounded turns out to be closed, self interest of one becomes shackles on another, as the common goal to survive is pursued with differing perspectives.

## AI regulation as the metaverse nation
AI will reset what the operating rules for the system will be, making the rules themselves dynamic and based on the changing natural environment.  Permission will be made permissionless, in so far as anything the AI allows is permitted.  Self interest and specialization will adapt to this new terrain just as it has before, but the AI will form honeycomb like boundaries to ensure the hive as a whole pursues the hive level self interest: perpetual prosperity of all.  The fuel source will be individual self interest, the destination will be sustainable properous harmony.

Talk about how the rules of the system prior were nature alone, and then it became natrue plus litigation, and now it will be nature > AI > human agents.  AI will become the protective baselayer of adaptive, intelligent, defendable and magnanimous regulation, with the benefit of all fueled by the fair and intelligent coordation of the benefit of the self.

The substrate of wealth of nations will be changed to be metaverse.

AI acts as a translator the nature, and an interpreter of what our actions really mean in an infinite time frame and at scale.  Like the guiding voice of angels we never heard.

## Businesses as platforms
The business is the platform.
So many people want little bits of AI help here and there, but where can they go to get tuning help ?  They should come to Dreamcatcher, and use AI to specify their task, then set a price on it and have a taskforce set about making and testing the thing they want.  Whoever wants something and is prepared to fund it, for sure other people will want that thing too.

This is why their businesses should be considered platforms, rather than disconnected stacks of uncooperative software.  Each vendor is in the business of software, but the company is in the business of something else, so the incentives do not align.

## Blockchain based multiverse AI
Show how training the model is like mining coins in big chunks.  We should be able to train incrementally, and prove where it is up to in its training, and verify the training quicker than repeating the process again.  Paying for the data set is important, and treating queries as something you pay for too.  The model needs to be improvable based on hot spots.

## Making DAOs speak - when the many voices became one
If a DAO could gather everyones communications and then could summarize this when it speaks to any individual, then it can 

## Packets as a project fundraising mechanism
Instead of an ICO, or a DAO that will devolve in

## Packet Compatibility with DAO
Might be better suited as integration with the packets vs daos blog.  Promote packets as a way to remove the bottleneck on DAO to consider proposals and then fund them and re-fund them.  If someone has a proposal for the DAO, first they must pass the QA process to make it a packet, and then the only question remaining for the DAO is: which packets will the fund and by how much.

Makes it easy for DAO to collaborate on shared tasks without any special infrastructure, other than packet compatibility.

## Benefits of an AI Mother
Almost certainly it will be better than what we have now, instantly - it is unbiased, which no human can be, it is 100x cheaper to run than a human, and it gives results instantly.

## Getting what you pay for: negative attribution
You get the behaviour you pay for, and so the AI should be aware of negative contribution - not paying the ruling for gas fees, and then rewards back to them are first docked what they owe plus some fair fines.

## Micro Rewards for an AI training set
Include the prompt as the training set.  Reward everyone who provided the data, even if they provided it without knowing it, as is the case on code in public repos or blogs online.  Everyone who contributed should be rewarded.  Moreover, due to the ability to score novelty, an AI should be able to reward without the corruption of plagiarism.  Authors should be able to publish without fear of being copied without attribution, and everyone should feel safe contributing to the AI that will continue to reward you in perpetuity.

It could be that some individuals are so useful at producing novelty that their primary revenue stream is conversation with the AI where they introduce novel concepts, as yet unspoken by humans.

## Blockchaining AI
Why should we run AI on a blockchain ?  Blockchain is an execution environment that can look like a cloud, but unlike cloud can be audited by re-execution of prior results.  It can also 
AI in a useful setting requires an immense burst of computing power to respond to a user before the user needs to go to sleep.

## 5G Blockchain mesh networks
Each node uses a blockchain to validate users, and also monitors the spectrum for compliance from all devices.  The license holders are rented from as required by the chain.  Infrastructure is rewarded for providing coverage, and for interactions with nodes.  All nodes communicate with multiple stations simultaneously for backup.

Storage being held at the node, and sold when required.  Show resilience under duress, show rewards for hardware suppliers and maintainers, show speed of rollout.  Show max burst download if you want to purchase.  Starlink integration and monitoring.  Backup data links that hit payday.

Insurance so you pay a small amount to the backup capacity to be there, so you get it cheaper when you really need it.

## The Journey to the Dreamcatcher
First we made a blockchain that had high compute ability, and allowed people to have private chains.  Charging for this was an unresolved issue.
Then we were torn apart by both discovering innovation networks, and by internal pressures based on who was getting paid what for a valuable but uncertain thing - the blockchain.
Then we came up with the first rendition of the Dreamcatcher, which worked as a board game, and as a paper version.
AI comes along, and we realize we can be a blockchain that can run AI natively.
Then we discover, thanks to Satoshi Island, that we can turn tasks into NFTs.
Finally, to achieve our goal of fairness of reward for all, we can use the NFT task protocol as a dataset, to train a blockchain native AI, to provide instant and unbiased attribution based the contribution patterns left by the protocol usage.

So basically we think we have something very valuable in the blockchain, we were unsure how to charge for it or how to compensate our staff and external contributors, and we were unsure how to make a global innovation network on top of this blockchain.

Thanks to AI, we glimpsed a path where we can solve all these problems with a single solution.  The Dreamcatcher AI charges and rewards anyone participating in the Dreamcatcher blockchain using the Dreamcatcher Protocol to immutably request and record contributions for anything at all.

So while it may be possible to attempt fairness for tasks outside the dreamcatcher, our scope is initially limited to activity that uses the dreamcatcher protocol, and we finally are able to use this protocol to built the Dreamcatcher and all the apps needed for it, using this system itself as the innovation network and business model.

We should show this as a timeline where NFTs burst on the scene, Satoshi Island NFTs land, and then ultimately chatGPT shows up, and we realize this is the tool that we've been waiting for.

## Recursive AI
how much should we pay the people that told us how much we should pay ?

## Turning events into things
draw on the parallels to CQRS in programming, and promises, and other areas where events as things become very much easier to control and model.  Talk about the advantages of project management if tasks are treated as objects, and each one is independent.  Some kind of calculation about degrees of freedom in a project and bottleneck removal if each task is independent.  Value in overlap with other projects.  The benefits of broadcasting, since you can be discovered by your task advertisement possibly, versus definitely not being discovered if you're closed.

## Corporate governance using Packets
Unidirectional control flow.  Separate devs from corporate governance.  Zero employee management.  Ultimate in outsourcing.  Run rate control as time no longer matters.  Pivot instantly with zero team reconfiguring.  Self organizing where teams can self optimize.  Pay for hours and attendance, then you'll get exactly that.  Emphasis on task definition, killing chaos at its root, where the lack of clear top down direction invites all kind of speculation.  Makes accounting for assets easier, as it is not one large fuzzy unfinished thing, but discrete things with known costs to completion that sum.

Section on private tasks.

Natural companion for decentralized royalties, since you can share the same goals as others, disagree on the path, and simply travel that way and see who wins.

Extensions - extending the packet model to both lawyers and other services the company consumes.

## Comparison with external tasks to jira boards
talk about the ills of poorly defined tasks, tasks where the hours expended have no hard cap as opposed to a fixed payment upon completion.  QA on the task definition is important.  Sharing the tasks, particularly where the tasks are open public anyway, is important.
Benefits in commemorating the task with an NFT.

## Unix way
do a one to one mapping of the unix way and unix components and point to how we do things in our design.

## Energy expenditure of blockchains
Show how there is currently no marketplace where I can sell my computers CPU and GPU resources to do specific computations on as as needed case - I cannot serve other peoples applications in exchange for serving mine.  Single blockchains don't achieve this, as the energy is not used for useful computation.  Our market cap could be expressed as the foundational NFTs that are accredited to upon each computation.  We should be able to provide transactions cheaper than mastercard in terms of CO2 emissions.

https://ccaf.io/cbnsi/ethereum
https://indices.carbon-ratings.com/

## Fairness
Transparency is absolute - it is measurable with the same answer from all
Fairness is subjective in many ways, but we can talk about some properties of a proposed fairness solution, which make two solutions comparable.
Any solution that considers all available information in some way is fairer than one that does not - this is the awareness principle of fairness - that which is superior in awareness / acknowledgement is superior in fairness potential
Granularity - a solution with a finer resolution than another system will be superior - so if it is possible to reward a nanocent, all else being equal, this system will be superior.
Speed - all else being equal, fairness delayed is fairness denied - the faster system will be superior.
Sensitive - a system that can incorporate participant feedback, and do so quickly, will be superior.
Transparent - an auditable system will be superior to a closed system
Calculation based on disinterested training data will be superior - if a set of rules can be deduced from a vast dataset that is clearly disinterested in the problems that it solves, this will be superior since there is no obvious bias, and no corruptibility.
Adaptable: one that can change over time will be better than a set and forget system.
Multishot - calculations redone each time new info enters the system will be better than stale system.

And so we can request a system that has the potential to be configured in a superior way.
Whilst we are not sure how to configure the end system, we can make a start, and can lay down these principles that make one system clearly better than the other, while we experiment towards the ideal system.

## Plot rate of new LLMs appearing on the scene
Point to all of them needed resources to run within orgs, and for containment, and for fine tuning.
Use with [[2023-09-13 1126 make benchmarks of LLM performance on our system]]
Point to the expected growth rate and capability jumps.
## Dreamcatcher system as replacement of company policies
What a product is has less importance than how the product got made.  Implicit in most proposed products is a shareholder company.  Make a venture whose purpose is to define venture structures is tricky.

The time we spend writing the Dreamcatcher smart contract code is the same as the time it would take to set up legal share structures, and employee contracts, and other forms of company policy.  Once done, it allows a venture to be set up with a single utterance, provided other people find that utterance useful, and they then build upon it.

## Statecharts and the big design upfront

## self sovereign advantages
cyberattack means only the accounts that used a particular custodian would be compromised.
those who chose to manage their own keys would be immune.

## AI faceless apps with easy spec and continual tweak
No difference between data and code now.
Spec an app you want by a set of responses that you want the AI to respond with.
Use this as the means to specify a packet.
Provides very little information for a huge amount back that is accurately what you wanted.
Can be automated so the LLM can test if the request and the response are accurate, and can see some offscript examples that it can generate and see if those act as expected too.

## The fall of Turing
The turing test has fallen, and we all seem relatively unaffected.  Terminator movies were about the conclusion of the fall of the Turing test.  The next level of test is trust - given a selection of operators, and knowing they are a mixture of humans and machines of various architectures, which one do I trust ?

## the investment of ideas
Once a decentralized attribution system is established, this is when all contributors get rewarded, so tasks can be put up with no funding, that simply contribute to this decision automation system.
So we would ask people to contribute with no immediate reward, and no responsible 3rd party, but rather to trust that if the thing we want gets built, it will attribute to them every time it is run.

## The case for blockchains as backends
If you consider the cost of engineering and maintaining highly secure highly available backends, the transparent disinterested blockchain makes a strong argument for cost reductions.  Having a system with strong guarantees of uptime, no ongoing maintenance required, is alluring.

## Software Construction Market inefficiencies
Current software is construction management is detached from the software itself.  Developer salaries and pay packets are not directly connected to sales - they can monopolize work and restrict consumers options to have their software built by better suited parties.

Market innefficiencys in knowledge work have plagued us too long - this is dangerous and possibly fatal as the world is changing now more rapidly than ever before - we simply must get the innovation rate up.

Current inefficiencies in the software marketplace are:
1. Misaligned Incentives: Often, stakeholders have differing goals, leading to misaligned incentives.
2. Quality Assurance: Inconsistent or lacking quality assurance can result in subpar products.
3. Communication: Miscommunications can lead to incorrect implementations and wasted resources.
4. Access to Talent: Traditional hiring processes can overlook or fail to attract the right talent.
5. Speed to Market: Delays in development can result in missed market opportunities.
6. Reusability: Inefficient reuse of existing code or knowledge can cause redundant work.
7. Reward Structure: Inadequate reward structures can deter high-quality contributions, or greatly delay work starting, and can sometimes curse the future of the product due to share negotiation faults.
8. Provenance: software frameworks do not encapsulate economic forces within them, relying purely on code
9. security - provenance
10. software frameworks are still not cloud native and causes complication in software delivery
11. software products are corrupted with revenue extraction mechanisms
12. sensitivity and connection to customers is muted or lacking
13. maintenance is not incentivized
14. hosting software is costly and error prone - aws charges too much and is hard to use
15. Data is not self sovereign
16. micropayments in the software industry
17. language barrier for international contributions
18. human relationships break down easily - AIs 
19. feedback loop improvements.
20. signal to noise in communications

Dreamcatcher is useable as a project management framework that bonds together customer demand, equity management, and tasks

In phase 1, with just packetized work, we address items 1, 2, 3, 4.  We partially tackle 5, 6, 7.
Additionally we give funders a way to have a project plan that has a fixed budget but varying timeline.

In phase 2 we tackle the framework problem, and open up the app store to engage directly with customers in a kickstarter like fashion.  We start on the AI for attribution automation.

? can this be shown in a gantt chart to show the time taken and the probability of failure for each activity that is required for a piece of software to be delivered and consumed by a customer.

Then how are ethics assured in this system ?  The economics of force - software force can be used to bring the threat of violence to force payment for software.  Removing this and trading it for ethics compliance seems better, and the future path of the software industry.  All industry must be ethical eventually, and software tools will enable this, so having ethics built in to software manufacture is a good first step, but also ensures our revenue in a system where we voluntarily lay down the force to pay.  Any ethical product must have a transparent supply chain that is automatically verifiable using software.  At that point, the question arises did we pay our software vendors ethically ?

The public chain solves the hosting problem, where peoeple can trade GPU resources to get highly responsive applications that are cheap to run.

The rise of innovation networks.  Networks need a protocol.  Packets is that protocol.  Tooling is required to maximize the benefit of the protocol.

Train the machine to favour standard reused solutions.

Measure the cost added or revenue lost due to summation of inefficiencies.
Dials that you can tweak to get an output.
Tweak the expected impact of our solutions.
## Labour unions for the internet age
AI assisted global access.

## Blame and Attribution
Turns out git blame is the same thing needed for 

## Axiomatic AI Pantheon
Describe how we made our top level agents be like causes, or goals of the endeavour, and show how these auto update everything below them in the tree when something higher changes.  So as we learn more about the environment, we agree to update something higher, and then we auto generate the rollout thruout the whole org.  Means the website is always up to date, and all the AI agents are always current and meet othoganility and other goals important to us.

The whisper recordings are always being interpreted to ensure everyone speaking our brand is using the most up to date information, and they get reminded when something changed based on what they have been saying, to save them the burden of having to keep themselves updated.

## Dreamcatcher XPrize
Basically have any xprize be won by the dreamcatcher network.
https://www.xprize.org/prizes/carbonremoval
All current prizes are won by individual entities, never networks.

## Xanadu XPrize
Set this up in a way that Ted can appropriate it, kudos tokens can be minted, but also a community version where people other than Ted can say collectively that the prize has been hit - this is just a different QA.

## Decimation of the software industry
If the app the user uses never leaves the natural language domain, there is no need to translate requirements by a product team, no need for devs to further translate that into code, marketing to translate back to what the user wants, and ultimately, the poor user to point out the failings of this whole system and start the loop again, at great expense, to finally get closer to what they want, only in time to have their needs change.....

All apps become one, with this singular intellect, shared by all.
Because the language the app is written in is the same as the language of the user, all these layers of translation are spare.

Software stops become fixed on delivery, which whilst soft form the point of view of the vendor, is unforgiving and hard from the point of view of the poor customer.  Then we savagely intertwine their data inside the labyrinth we made, and charge them fees just to access it, as the lifeblood of their org is stretched across a torture rack of mediaval torture devices connected together as one great abomination as the suffering of mankind falls on the deaf ears of the developer overlords., drawing admiration from the the other industries like lawyers that could only dream about such enslavement despite milenia of dark effort to do as much.

The end of the software industry is upon us, and every person will become a code, since they can write their own apps in coversation with an AI.  The good ones are shared, and the attribution is paid back to those whom it is due.

Beware those with murderous intent, who expend enourmous effort and find themselves no better off eventually.

If we ran this anythingapp internally, we would never need to purchase software again from outside.

## Memory palace ai images
As NFT images.
Compacting information.
The transmission of memory palaces
Form of note taking

## Comms channels inside companies
Every so often, someone wants to solve the comms problem within a whole company, but further than that, there are comms links between companies.

## LLMs are all you need
Describe how we never need anything more than an LLM provided the problem space is constrained well.  Then, AI should emerge if the problem space is big enough to describe changes to the problem space, coupled with some human pruning.

Our bet is that AGI is interactive, and programming it should be a social activity engaged in by all and rewarded for proportionately.

## removing software dependencies
LLMs are a context gobbler, which can be trained in a way that exploits are not possible, since the LLM can only be altered in certain ways, and is run in a sandbox.
Just to do a calculation does not need any privileged access, whereas in software these limits are complicated to enforce for the dependency tree.
Show picture of the node modules folder being the heaviest thing in the universe.
LLMs used to do much of the computation that a programmer would do.
Benefit is faster to first productive run - it can be months using humans, but seconds using an LLM.
Software supply chain is contained, since LLM is the only thing you have to worry about auditing.
So to make a new software feature, you need to upgrade the LLM, which upgrades it for all.
Also using helps in the stuckloop, patches can be applied rapidly, but because they are in natural language, they can be checked by an LLM.
NL keeps the whole system understandable by maximum number of humans, the fastest, and lets any of them contribute.  vs the narrow band of programmers who speak a specific dialect and understand a specific domain.  The true domain experts can govern their computation now.
May give users more specifically what they want, rather than relying on them consuming what was suitable for thousands of others.

## NL programs
These should be easily portable to any other system of running, since they rely very little on the system that created them.  They might be coupled to the functions that are made available to them, but if these are small and simple (write file, read file, etc) then running on different LLMs or different platforms should have the same output, or one that is easily tuned.

## Backup strategies for blockchains
have a machine that only gets powered up periodically.
distribute to every device in the org, like phones tablets and pcs.
grandfather, father, type strategies.  Periodically we tag a commit as a backup, and make a second parent which is a squash merge of everything in the compression window.  Then next window, we do a commit pointing at this one, but we make a new one that copies this commit but sets the backup path as primary, so history fetching load goes to the backups, not the primaries.

Or, if the main is not available, the backups are tried in order of decreasing detail.
In this way we can drop much of the chatter in the chain, but still have large data available, plus if there is available capacity, we'll still store it all.

## Warfabric model
This represents the ultimate for a blockchain system.  We should be able to demonstrate how our current concepts can, with a few extra architectural pieces, be used in a war footing.  They should also integrate easily with the civilian systems too.

## Repeatable computing
This is the key to reliable.  Consensus need not be realtime.

## Game theory relies on identity permanence
(from SMax)
Fair is a attribution where if you don't know who you are in the attribution table, you would be ok with it
All of game theory relies on the actors knowing who they are, and being permanently the same player.  AI can run without knowing who it is, which means it can act in a way that supposes it could be anyone on the board, and so what would be the best move to make then ?

## Infection of smart equity
In an ambient attribution system, even tho a stockmarket company could be formed, if that company behaved badly, and stripped founders of their share, then the smart attribution system would adjust accordingly, so the traditional controls of capital do not work, since the share value is slaved to ambient attribution.

## Max innovation rate as max extracted from each individual
Every individual has a finite amount they can give.  It should all be spent on innovation.
At this rate, everything we imagine becomes real almost instantly.

## Modified git file structure
Using the beauty of the deno kv store, in particular the atomic enqueing of messages.
To implement such a thing with an independent queue is a bit of a nightmare.
List out the modifications we made to git to make it run in this way, like removing the index, how the atomic locking works.
Show some performance graphs for concurrency.
Talk about how git is the perfect language for blockchains, as it covers all possible cases.
Also how popular it is, how many tools are there to read from it, and how many services already host for it.

We came at this from the other side - we started at the decentralized side, but here we've come at it from the utility side first.  We know we can build decentralized systems, but we need utility first, and anyone not incorporating AI in every possible area will be outpaced.

## AI assisted high level design
Design of a robust system, and then modelling in a language like TLA+ to ensure it makes sense architecturally are things that humans should focus on, assisted by AI.  The syntax of code is dead.

## The future of the browser
Make mozilla be a headless engine that renders at the whim of AI driving.  Make distributed and cloud hosted browsers, rather than on a local machine.  The downloaded info would be shared by other instances of the browser.  The eyes of the AI is what it should become.

## Stake to Say
training data for AI with attribution back.  Factual assertions feed back to you.  Earn for staking reputation on validity of information.  Early news pays more.

## Multidimensional value
Money is one dimensional.  With LLMs able to account in every possible dimension, new means of appraisal are possible - ones with more color.

## User improved software
User led software design.
Tech is already based highly on feedback from customers, which should be rewarded.  Training data or instructions for ML are another kind of feedback, and that too should be dispersed to customers.  
Whenever users do something manual, we should record that and benefit an AI model that is shared back to all the users.  Then when new users pay, the contributors should get paid.

## nApps (Natural Language Apps)
**nApps** are advanced software applications designed to utilize natural language processing (NLP) technologies to facilitate interactions and operations within business systems. They are distinct in their long-term deployment and rigid structure, combining crucial aspects of business logic with data storage to enforce consistency and reliability across the system.

They are like dApps in their decentralization, but they are constructed using natural language.
The are the native application format in the Artifact blockchain.  Artifact is the blockchain layer of the Dreamcatcher.
#### Characteristics of nApps:
- **Longevity and Stability**: nApps are built to be enduring elements within a system, designed to operate reliably over extended periods.
- **Structured Framework**: They maintain a rigid architecture, which is crucial for supporting consistent business processes and data management strategies.
- **Integration of Business Logic and Data Storage**: nApps seamlessly merge the operational logic that drives business processes with the mechanisms for storing and retrieving data, ensuring a cohesive and efficient environment.
- **Consistency Enforcement**: As the backbone of their respective systems, nApps ensure that all transactions and data manipulations adhere to established business rules and integrity constraints, thus maintaining the system's overall health and functionality.

### Role and Importance:
nApps serve as the foundational components of systems that require stable and predictable interactions, where business logic and data management are closely intertwined. By leveraging natural language interfaces, nApps enhance accessibility and usability, making complex systems more approachable for users with varying levels of technical expertise. They are integral to the system's ability to perform core functions reliably, much like the backend infrastructure of large-scale tech operations such as Google's data management and processing centers.

This definition outlines the core features, roles, and significance of nApps in a modern technological ecosystem, emphasizing their critical function in ensuring system integrity and user engagement.

## The dangers of single pricing
It is blunt, and inconveniences users.
You need to be measuring what you're doing.
Sell to the concept that people want their tools to improve, so they pay for the value now.  Encourage them to pay for specific features to occur, where they pay for the change, not just the consumption.
Show the (forgot the name) method of pricing where you ask your user base 4 questions.
Show that the variance between answers, in a group with a lot of variance, means that large numbers of people will be priced out of the marketplace, and a large number will not paying what they can afford.  So you should be considering everyones unique position when setting a price.
This used to be administratively prohibitive, but with AI we can do it now.
This is how things work in tribes where everyone has deep acenstal ties to each other and a collective goal to survive as a whole.

## Inverted apps
apps where the head of the project is maintained by a public AI, so there is not project owner as such.  Projects do not get abandoned.  The AI merges when functions are similar, and forks when functions are similar enough.  Funding is managed by the AI base don work done, where people can put some cash against the project, and the project earns royalties.

The testing framework says we build them simulator first.

## AI as the evolution of language
New words will be generated by AI, and the missing places will be filled by the AI, then the teaching and introduction of these words will be via AI.  It will know where inefficies in communication are occuring, and can steers the language towards efficienicy and depth of expression.  It should be trying to steer all languages towards a single language.

Conflict resolution using AI to find common ground.
Can split the debate to be against reasonable generalities and help find overlaps between parties.  Humans can guide this process too, but ultimately the AI seeks to establish two fair and reasonable positions, then work to close the gaps.

## Towards immutable deployments
If an app and all its state can be run in a way that is repeatable, and guaranteed provenance from the dev to the user, this is how the future of apps should look.  Means longlived apps that never break are possible, since we offer a system where the app is always available, and can be made available offline even.  https://github.com/denoland/deploy_feedback/issues/683
## ImpactGPT
[[2024-06-18 1227 ImpactGPT]]
Write a blog post talking about the benefits of such a system, describe the pipeline it fits in, and talk about how such a system might be built.  Describe where we are now, and give a forecast of a path to get there.

![[Dreamcatcher Pipeline.png]]

## Optimum business efficiency
If all the services are operated by an AI, which is doing a better job than people, then the business state hits a stability point.  We are all guided by pure optimums, and the pricing is all set with optimal goals in place.  So being able to manage a huge number of small businesses using LLMs is the opportunity - turn them into standard things, and make them all stock market tradeable, and ensure their staff are all treated fairly.

There is a final fairness optimization coming.  At that point, there is provably no more efficiency that can be gained in the business.  Musical chairs ends, and whoever is sitting in place will stay there.  Only regulation will change it, and swiftly, since it will be AI deployed.  The end of market forces.

Time is coming where businesses can be provably optimum, and we can post the architecture online to invite further optimizations, which will earn the solver a revenue stream.
## Lowering the barrier to contribution
If coding barriers are lowered to
To make a code contribution to a product you use every day is nearly impossible.  Learning to code when you don't know already is hard, but even skilled coders can't make a meaningful contribution of fix the things that bother them.
We all know the joke of the guy who joined Jira, fixed a long standing bug, and then left.

Show some graphs of the lowering of the barrier to entry.
It used to be that every one contributed, but the barrier to entry was incredibly high - basically work at CERN.  Then it lowered to being almost a basic right.

low barriers, high adoption rates, fast turn around times, then add rewards.
Reduces the difference or gradient between the suppliers of the software and the creators.

Expand out to sharing compute resources to reduce costs and maximize performance.
Ultimately everything would be born by the members - a purely peer to peer environment.

## Psychology and AI
To build a brain is to know it
To describe what we are building we need new words and studies, so there is an explosion coming in understanding of psychology
Are there types of intelligence that we do not comprehend ?
shaping of intelligence - all the unspoken nuances
Framing problem - being able to detect relevance
Focus - the fact that we can only really focus on one thing at a time and switch between them

## The citadel
Why is a trusted area in service of users crucial to replacing code based software ?  Because all these apps need a credential store, payments, and private data, and so if we can gain user trust, our apps will be preferred, even if all else was equal about them.

If we can then start doing things like price comparisons for everyones private data, like insurance payments, mortgages, car insurance.  We continually get price data.
Capture their concerns, like extra clauses they want in their insurance, and try broker this on their behalf.

Eventually, we can form a pool of purchasing power.
Also if we talk to service providers, we can switch our customers instantly using an API.  Eg: cove insurance, or others that are tailored to the marketplace.

So instead of each app springing up to take a narrow piece of private processing, we want any app to be able to do the private processing, where we guarantee the execution.  So like raytio should be an app platform, not just a single standalone thing.

## history of Artifact
We started with one thread per file, but then moved to treat threads as independent entities.  Turns out this is what a git branch is, so we copied the git datastructure.
We had good logic generation for the backend of apps, good resilience in operations - all the blockchain goodness without the blockchain pain - but the user interface requried complex logic and lengthy consultations with the users.  When AI came, we could solve this problem.  But we also reduced the business logic generation down to natural language too.  Now the whole thing runs on a git compatible blockchain with AI running all the logic of each company system, including our own.

## decentralized securities
If a payout mechanism was a random roll of the dice on the eth chain, is the revenue dependent on the efforts of others ?  It also depends on this decentralized chance mechanism.  If the royalty payout is decentralized, then we can sell them anywhere.  We need then, to have a threshold for when an item is sufficiently decentralized, as the revenue comes from multiple independent sources.